{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SoFLIx-rF-N"
      },
      "source": [
        "# Bat4RCT: A suite of benchmark data and baseline methods for text classification of randomized controlled trials\n",
        "\n",
        "Train and test heuristic, machine learning, Convolutional Neural Networks, and BERT-based models to classify text data related to RCT (randomized controlled trial)  \n",
        "<br>\n",
        "\n",
        "Author: Jenna Kim  \n",
        "Last Modified: 2023/3/10 \n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "How to run the code:  \n",
        "\n",
        "1. Place this jupyter notebook (\"Bat4RCT_Main.ipynb\") in the same directory where input data file (\"rct_data.txt\") and \"modules.py\" file are located.  \n",
        "2. Run the \"Setup: Import libraries and functions\" for set up  \n",
        "3. Select one of the following sections for each model type and run the code:\n",
        "    * ML model:  1. Machine Learning (ML) model  \n",
        "    * CNN model:  2. Convolutional Neural Networks (CNN) model  \n",
        "    * BERT model:  3. BERT-based model  \n",
        "    * Heuristic model:  4. Heuristic model  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjDQGl91wA78"
      },
      "source": [
        "# Setup: Import libraries and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCQmTfQRrF-S"
      },
      "outputs": [],
      "source": [
        " from modules import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install package if not already installed\n",
        "\n",
        "#!pip install imbalanced-learn==0.8.1\n",
        "#!pip install scikit-learn==1.0.2"
      ],
      "metadata": {
        "id": "aBdq7f5P3thY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtuA57AQwTjA"
      },
      "source": [
        "# 1. Machine Learning (ML) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYt_qbIcrF-a",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "    \n",
        "    ######################################################\n",
        "    #############  1. Set Parameter Values  ##############\n",
        "    ######################################################\n",
        "\n",
        "    #############  1-1. Input file name & which column  #############\n",
        "\n",
        "    input_filename=\"rct_data.txt\" \n",
        "    column_name = \"title\"                                      # 'title' for title text; 'abs' for abstract text; 'mix' for title + abstract text\n",
        "\n",
        "    #############  1-2. Data size change?  #############\n",
        "\n",
        "    datachange_on=0                                            # 0 for no change; 1 for change of data size\n",
        "    \n",
        "    ## Set the following parameters when datachange_on=1\n",
        "    ## class balance (1:1)? \n",
        "    balance_on=0                                               # 0 for no balance; 1 for class balance (1:1)\n",
        "    balance_sample_on=1                                        # 0 for no sampling; 1 for sampling\n",
        "    balance_sample_type='under'                                # 'over'(oversampling); 'under'(undersampling)\n",
        "    balance_str = 'balance' + str(balance_on) + '_'\n",
        "    \n",
        "    ## data increase?\n",
        "    ratio_on=0                                                  # 0 for no ratio; 1 for using ratio list\n",
        "    ratio_list=[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, \n",
        "                0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]  # ratio for data size\n",
        "    \n",
        "    #############  1-3. Data sampling applied?  #############\n",
        "    \n",
        "    sampling_on=0                                              # 0 for no sampling; 1 for sampling\n",
        "    sampling_type='over'                                       # Use when sampling_on=1; 'over'(oversampling), 'under'(undersampling)\n",
        "    \n",
        "    #############  1-4. Which model to use?  #############\n",
        "    \n",
        "    model_type='LR'                                            # 'LR'(Logisitic regression); 'SVM'(SVM); 'GB'(Gradient Boosting);\n",
        "                                                               \n",
        "    #############  1-5. Evaluation & probability file  #############  \n",
        "    \n",
        "    eval_on=1                                                  # 0 for no; 1 for yes (confusion matrix/classification report)\n",
        "    proba_on=0                                                 # 0 for no; 1 for yes (probability output)\n",
        "    \n",
        "\n",
        "    ######################################################\n",
        "    ###############  2. Run Main Fuction  ################\n",
        "    ######################################################\n",
        "\n",
        "    if datachange_on:            \n",
        "        \n",
        "        for ratio in ratio_list:           \n",
        "            if sampling_on:\n",
        "                proba_file = \"result_ml_\" + balance_str + str(ratio) + \"_\" +  model_type + \"_\" + sampling_type + \"_\" + column_name + \".csv\" \n",
        "                eval_file = \"eval_ml_\" + balance_str + str(ratio) + \"_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".txt\" \n",
        "            else:\n",
        "                proba_file = \"result_ml_\" + balance_str + str(ratio) + \"_\" + model_type + \"_\" + column_name + \".csv\"   \n",
        "                eval_file = \"eval_ml_\" + balance_str + str(ratio) + \"_\" + model_type + \"_\" + column_name + \".txt\"\n",
        "            \n",
        "            run_ml(input_file=input_filename,colname=column_name, sample_on=sampling_on, \n",
        "                   sample_type=sampling_type,model_method=model_type, eval_on=eval_on, \n",
        "                   proba_file=proba_file,proba_on=proba_on,result_file=eval_file,\n",
        "                   datasize_change=datachange_on,sample_ratio=ratio_on,sample_balance=balance_on,\n",
        "                   balance_sampling_on=balance_sample_on,balance_sampling_type=balance_sample_type,\n",
        "                   ratio=ratio)\n",
        "    else:\n",
        "        if sampling_on:\n",
        "            proba_file = \"result_ml_all_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".csv\"    \n",
        "            eval_file = \"eval_ml_all_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".txt\" \n",
        "        else:\n",
        "            proba_file = \"result_ml_all_\" + model_type + \"_\" + column_name + \".csv\" \n",
        "            eval_file = \"eval_ml_all_\" + model_type + \"_\" + column_name + \".txt\" \n",
        "            \n",
        "        run_ml(input_file=input_filename, colname=column_name, sample_on=sampling_on, \n",
        "               sample_type=sampling_type, model_method=model_type, eval_on=eval_on, \n",
        "               proba_file=proba_file, proba_on=proba_on, result_file=eval_file,\n",
        "               datasize_change=datachange_on, sample_ratio=ratio_on, sample_balance=balance_on,\n",
        "               balance_sampling_on=balance_sample_on, balance_sampling_type=balance_sample_type,\n",
        "               ratio=1)\n",
        "        \n",
        "    print(\"\\n************** Processing Completed **************\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Convolutional Neural Networks (CNN) model"
      ],
      "metadata": {
        "id": "5n4dlS55Wym6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTixa_JMIpPw"
      },
      "outputs": [],
      "source": [
        "## Check if GPU availability\n",
        "\n",
        "# TensorFlow supports running computations on a variety of types of devices, including CPU and GPU. They are reperesented with string identifiers. \n",
        "# For example:\"/device:CPU:0\" (CPU of your machine), \"/physical_device:GPU:0\" (GPU visible to TensorFlow)\n",
        "# TensorFlow code, with Keras included, can run on a GPU by default without requiring explicit code configuration. \n",
        "# If both CPU and GPU are available, TensorFlow will run the GPU-capable code unless otherwise specified.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"GPU device: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# check GPU memory and & utilization\n",
        "\n",
        "#!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install packages to run CNN model if not already installed\n",
        "\n",
        "#!pip install tensorflow==2.11.0\n",
        "#!pip install keras==2.11.0"
      ],
      "metadata": {
        "id": "nfrX22Cz3Prq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "    \n",
        "    ######################################################\n",
        "    #############  1. Set Parameter Values  ##############\n",
        "    ######################################################\n",
        "    \n",
        "    #############  1-1. Input file name & which column  #############\n",
        "    \n",
        "    input_filename=\"rct_data.txt\" \n",
        "    column_name = \"title\"                                      # 'title' for title text; 'abs' for abstract text; 'mix' for title + abstract text\n",
        "    \n",
        "    #############  1-2. Data size change?  #############\n",
        "    \n",
        "    datachange_on=0                                            # 0 for no change; 1 for change of data size\n",
        "    ratio_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]   # ratio for data size\n",
        "\n",
        "    #############  1-3. Evaluating model performance?  #############     \n",
        "    \n",
        "    eval_on=1                                                  # 0 for no; 1 for yes (confusion matrix/classification report)\n",
        "    \n",
        "    #############  1-4. Hyperparameters for CNN  #############\n",
        "    \n",
        "    MAX_LEN = 150                                              # 150 for title; 512 for abs (Consistent with BERT parameters))\n",
        "    BATCH_SIZE = 16                                            # Batch size: 16 or 32\n",
        "    EPOCHS = 4                                                 # Number of epochs: 2,3,4\n",
        "\n",
        "    \n",
        "    ######################################################\n",
        "    ###############  2. Run Main Fuction  ################\n",
        "    ######################################################\n",
        "\n",
        "    if datachange_on:               \n",
        "        for ratio in ratio_list: \n",
        "            eval_file = \"eval_cnn_\" + str(ratio) + \"_\" + column_name + \".txt\"\n",
        "            \n",
        "            run_cnn(input_file=input_filename, \n",
        "                    colname=column_name, \n",
        "                    max_len=MAX_LEN, \n",
        "                    batch_size=BATCH_SIZE, \n",
        "                    epochs=EPOCHS, \n",
        "                    eval_on=eval_on, \n",
        "                    result_file=eval_file, \n",
        "                    datasize_change=datachange_on, \n",
        "                    ratio=ratio)\n",
        "    else:\n",
        "        eval_file = \"eval_cnn_all_\" + column_name + \".txt\" \n",
        "            \n",
        "        run_cnn(input_file=input_filename, \n",
        "                colname=column_name, \n",
        "                max_len=MAX_LEN, \n",
        "                batch_size=BATCH_SIZE, \n",
        "                epochs=EPOCHS, \n",
        "                eval_on=eval_on, \n",
        "                result_file=eval_file, \n",
        "                datasize_change=datachange_on, \n",
        "                ratio=1)\n",
        "        \n",
        "    print(\"\\n************** Processing Completed **************\\n\")"
      ],
      "metadata": {
        "id": "bUTlEwyCYf85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. BERT-based model  "
      ],
      "metadata": {
        "id": "-7fQC0J5hKej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Check if there's a GPU available\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are {:d} GPU(s) available.'.format(torch.cuda.device_count()))\n",
        "    print('We will use the GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "\n",
        "## Check GPU memory and & utilization\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "7OGkZiTwhOaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Clear the occupied cuda memory for efficient use\n",
        "\n",
        "#import gc\n",
        "\n",
        "#gc.collect()\n",
        "#torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "t_90QOHnCQff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Install packages to run BERT-based model if not already installed\n",
        "\n",
        "#!pip install transformers==4.15.0\n",
        "#!pip install torch==1.5.0"
      ],
      "metadata": {
        "id": "gps8IZuahPDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "    \n",
        "    ######################################################\n",
        "    #############  1. Set Parameter Values  ##############\n",
        "    ######################################################\n",
        "\n",
        "    #############  1-1. Input file name & which column   #############\n",
        "    input_filename=\"rct_data.txt\"    \n",
        "    column_name = \"title\"                                        # 'title' for title text; 'abs' for abstract; 'mix' for title + abstract\n",
        "    \n",
        "\n",
        "    #############  1-2. GPU setting    #############\n",
        "    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    #############  1-3. Data size change?   #############\n",
        "    datachange_on=0                                            # 0 for no change; 1 for change of data size\n",
        "    \n",
        "    ## Set the following parameters when datachange_on=1\n",
        "    ## class balance (1:1)?\n",
        "    balance_on=1                                               # 0 for no balance; 1 for class balance (1:1)\n",
        "    balance_sample_on=1                                        # 0 for no sampling; 1 for sampling\n",
        "    balance_sample_type='under'                                # 'over'(oversampling); 'under'(undersampling)\n",
        "    balance_str = 'balance' + str(balance_on) + '_'\n",
        "    \n",
        "    ## data increase?\n",
        "    ratio_on=0 \n",
        "    ratio_list=[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, \n",
        "                0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]  # basic ratio for data size\n",
        "    \n",
        "    #############  1-4. Sampling applied?   #############\n",
        "    sampling_on=0                                              # 0 for no sampling; 1 for sampling\n",
        "    sampling_type='under'                                      # Use when sampling_on=1; 'over'(oversampling), 'under'(undersampling)\n",
        "    \n",
        "    #############  1-5. Which BERT model to use?   #############\n",
        "    #pretrained_model_name = 'bert-base-cased'\n",
        "    pretrained_model_name = 'dmis-lab/biobert-base-cased-v1.1'\n",
        "    #pretrained_model_name = 'allenai/scibert_scivocab_cased'\n",
        "    \n",
        "    # load pretrained tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)  \n",
        "    modelname_string = pretrained_model_name.split(\"/\")[-1] \n",
        "\n",
        "    #############  1-6. Binary or multi classification?   #############\n",
        "    num_class = 2                                              # number of label class\n",
        "    \n",
        "    #############  1-7. Check token distribution for MAX_LEN value: commentize if not needed   #############\n",
        "    #print(\"\\n************** Token Distribution **************\")\n",
        "    #df_token = load_data(input_filename, column_name, record=None)\n",
        "    #token_distribution(df_token, tokenizer)\n",
        "\n",
        "    #############  1-8. Hyperparameters for BERT   #############\n",
        "    MAX_LEN = 150                                              # 150 for title; 512 for abs (Maximum input size: 512 (BERT))\n",
        "    BATCH_SIZE = 16                                            # Batch size: 16 or 32\n",
        "    EPOCHS = 1                                                 # Number of epochs: 2,3,4\n",
        "    LEARNING_RATE = 2e-5                                       # Learning rate:5e-5, 3e-5, 2e-5\n",
        "\n",
        "    #############  1-9. Evaluation & probability files   #############\n",
        "    eval_on=1                                                  # 0 for no; 1 for yes (display confusion matrix/classification report)\n",
        "    proba_on=0                                                 # 0 for no; 1 for yes (probability output) \n",
        "    \n",
        "        \n",
        "    ######################################################\n",
        "    ###############  2. Run Main Fuction  ################\n",
        "    ######################################################\n",
        "\n",
        "    if datachange_on:                  \n",
        "        for ratio in ratio_list:           \n",
        "            if sampling_on:\n",
        "                proba_file = \"result_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".csv\"  \n",
        "                eval_file = \"eval_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".txt\"\n",
        "                model_state_file = \"best_model_state_\" + str(ratio) + \"_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".bin\"\n",
        "            else:\n",
        "                proba_file = \"result_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + column_name + \".csv\"  \n",
        "                eval_file = \"eval_bert_ratio_balance/eval_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + column_name + \".txt\"\n",
        "                model_state_file = \"best_model_state_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + column_name + \".bin\"\n",
        "        \n",
        "            run_bert(input_file=input_filename, colname=column_name, sample_on=sampling_on,\n",
        "                     sample_type=sampling_type, tokenizer=tokenizer, max_len=MAX_LEN, \n",
        "                     batch_size=BATCH_SIZE, modelname=modelname_string, n_class=num_class, \n",
        "                     device=device, pretrained_model=pretrained_model_name, \n",
        "                     learning_rate=LEARNING_RATE, epochs=EPOCHS, model_file=model_state_file, \n",
        "                     eval_on=eval_on, proba_file=proba_file, proba_on=proba_on, \n",
        "                     result_file=eval_file, datasize_change=datachange_on, sample_ratio=ratio_on, \n",
        "                     sample_balance=balance_on, balance_sampling_on=balance_sample_on,                                      \n",
        "                     balance_sampling_type=balance_sample_type, ratio=ratio)\n",
        "    else:\n",
        "        if sampling_on:\n",
        "            proba_file = \"result_bert_all_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".csv\"  \n",
        "            eval_file = \"eval_bert_all_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".txt\"\n",
        "            model_state_file = \"best_model_state_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".bin\"\n",
        "        else:\n",
        "            proba_file = \"result_bert_all_\" + modelname_string + \"_\" + column_name + \".csv\"  \n",
        "            eval_file = \"eval_bert_all_\" + modelname_string + \"_\" + column_name + \".txt\" \n",
        "            model_state_file = \"best_model_state_\" + modelname_string + \"_\" + column_name + \".bin\"\n",
        "            \n",
        "        run_bert(input_file=input_filename, colname=column_name, sample_on=sampling_on, \n",
        "                 sample_type=sampling_type, tokenizer=tokenizer, max_len=MAX_LEN, \n",
        "                 batch_size=BATCH_SIZE, modelname=modelname_string, n_class=num_class,\n",
        "                 device=device, pretrained_model=pretrained_model_name,\n",
        "                 learning_rate=LEARNING_RATE, epochs=EPOCHS, model_file=model_state_file, \n",
        "                 eval_on=eval_on, proba_file=proba_file, proba_on=proba_on, \n",
        "                 result_file=eval_file, datasize_change=datachange_on, sample_ratio=ratio_on,\n",
        "                 sample_balance=balance_on, balance_sampling_on=balance_sample_on,                                      \n",
        "                 balance_sampling_type=balance_sample_type, ratio=0.1)\n",
        "        \n",
        "    print(\"\\n************** Processing Completed **************\\n\")"
      ],
      "metadata": {
        "id": "xvMCTnzNw5Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Heuristic model"
      ],
      "metadata": {
        "id": "td4AabPNjHCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "    \n",
        "    ######################################################\n",
        "    #############  1. Set Parameter Values  ##############\n",
        "    ######################################################\n",
        "    \n",
        "    #############  1-1. Input file name & which column  #############\n",
        "    \n",
        "    input_filename=\"rct_data.txt\"  \n",
        "    column_name = \"title\"                                        # 'title' for title text; 'abs' for abstract text; 'mix' for title + abstract text\n",
        "    \n",
        "    ############# 1-2. Evaluation applied?  #############\n",
        "    \n",
        "    eval_on=1                                                    # 0 for no; 1 for yes (confusion matrix/classification report)\n",
        "    \n",
        "    ############# 1-3. Term list for keyword matching   #############\n",
        "    \n",
        "    keyword_list = ['RCT', 'RCTs', \n",
        "                    'randomized controlled trial', 'randomized controlled trials', 'randomised controlled trial', 'randomised controlled trials', \n",
        "                    'randomized trial', 'randomized trials', 'randomised trial', 'randomised trials',\n",
        "                    'randomized clinical trial', 'randomized clinical trials', 'randomised clinical trial', 'randomised clinical trials',\n",
        "                    'randomized controlled', 'randomised controlled', 'radomized clinical', 'randomised clinical',\n",
        "                    'randomized', 'randomised', 'clinical trial', 'clinical trials', 'controlled trial', 'controlled trials']\n",
        "    \n",
        "    \n",
        "    ######################################################\n",
        "    ###############  2. Run Main Fuction  ################\n",
        "    ######################################################\n",
        "\n",
        "    output_file = \"result_baseline_heuristic_\" + column_name + \".csv\" \n",
        "    eval_file = \"eval_baseline_heuristic_\" + column_name + \".txt\" \n",
        "            \n",
        "    run_heuristic(input_file=input_filename, \n",
        "                  colname=column_name,\n",
        "                  keywords=keyword_list,\n",
        "                  eval_on=eval_on,\n",
        "                  match_file=output_file,\n",
        "                  result_file=eval_file)\n",
        "        \n",
        "    print(\"\\n************** Processing Completed **************\\n\")"
      ],
      "metadata": {
        "id": "ZhLI51q_jHpz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}