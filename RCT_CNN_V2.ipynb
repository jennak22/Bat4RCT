{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NFQleLIQc6F"
   },
   "source": [
    "# RCT Text Classification using CNN   \n",
    "\n",
    "This code is to build a pipeline to classify RCT (Randomized Controlled Trials) on Pubmed texts using a CNN (Convolutional Neural Networks) deep learning model.  \n",
    "  \n",
    "    \n",
    "Author: Jenna Kim  \n",
    "Created: 2020/10/31  \n",
    "Last Modified: 2022/10/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CcSygkhDbB6"
   },
   "source": [
    "## Reference  \n",
    "* A simple Convolutional Neural Network summary for binary image classification with Keras  \n",
    "https://medium.com/swlh/a-simple-convolutional-neural-network-summary-for-binary-image-classification-with-keras-d58caf2f84a4  \n",
    "* Can you run Keras models on GPU?  \n",
    "https://www.run.ai/guides/gpu-deep-learning/keras-gpu  \n",
    "* How can you use GPUs with TensorFlow?  \n",
    "https://www.run.ai/guides/gpu-deep-learning/tensorflow-gpu  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY3m9eQWQc6K"
   },
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJiVCXKOy1iw"
   },
   "source": [
    "## 1-1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxyXO-krQc6K",
    "outputId": "9a7e4856-162b-48a4-b659-529ccc9687a1"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9UhXfGky7TE"
   },
   "source": [
    "## 1-2. Check GPU settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use \"conda_amazonei_tensorflow2_p36\" kernel to run this code in AWS Sagemaker. If not setup, you can find it go to Kernel -> Change kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list\n",
    "\n",
    "#!lspci | grep -i nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lWr7vKYQc6T"
   },
   "outputs": [],
   "source": [
    "# check the version of Tensorflow and Keras\n",
    "# Tensorflow (ver 2.3.4); Keras (ver 2.4.3)\n",
    "\n",
    "print(\"Tensorflow version: \", tensorflow.__version__)\n",
    "print(\"Keras version: \", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPQAI_5ryYU-",
    "outputId": "0a43778c-55ed-4d26-e68e-283c5c9d1cef"
   },
   "outputs": [],
   "source": [
    "# check if gpu is available\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU device: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js_T9kAT1MY7"
   },
   "source": [
    "TensorFlow supports running computations on a variety of types of devices, including CPU and GPU. They are reperesented with string identifiers. For example:  \n",
    "\n",
    "\"/device:CPU:0\" : CPU of your machine  \n",
    "\"/physical_device:GPU:0\": GPU visible to TensorFlow.  \n",
    "\n",
    "TensorFlow code, with Keras included, can run on a GPU by default without requiring explicit code configuration. If both CPU and GPU are available, TensorFlow will run the GPU-capable code unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D_JcUe9zIG2"
   },
   "outputs": [],
   "source": [
    "# To see which devices your operations and tensors are assignend to\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "#a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "#b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "#c = tf.matmul(a, b)\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU memory and & utilization\n",
    "#!nvidia-smi\n",
    "\n",
    "# To check the GPU memory usage while the process is running\n",
    "# open a terminal in the directory (Go to New-> Terminal) and type the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBuQ-mWHQc6L"
   },
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNB_aO7oQ_ZL"
   },
   "outputs": [],
   "source": [
    "def load_data_txt(filename, colname, record):\n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file\n",
    "    colname: column name for texts\n",
    "    record: text file to save summary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. Read in data from input file\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", encoding='utf-8', header=None, names=['pmid', 'pubtype', 'year', 'title', 'abstract'])\n",
    "    \n",
    "    # No of rows and columns\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record) \n",
    "    print(\"No of Rows: {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    ## 2. Select data needed for processing & convert labels\n",
    "    df = df[['pmid', 'title', 'abstract', 'pubtype']]\n",
    "\n",
    "    ## 3. Cleaning data \n",
    "    #Trim unnecessary spaces for strings\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: x.strip())\n",
    "    df[\"abstract\"] = df[\"abstract\"].apply(lambda x: x.strip())\n",
    "\n",
    "    # Remove null values \n",
    "    df=df.dropna()\n",
    "\n",
    "    print(\"No of rows (After dropping null): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record)\n",
    "    print(\"No of rows (After dropping null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    # Remove duplicates and keep first occurrence\n",
    "    df.drop_duplicates(subset=['pmid'], keep='first', inplace=True)\n",
    "\n",
    "    print(\"No of rows (After removing duplicates): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of rows (After removing duplicates): {}\".format(df.shape[0]))\n",
    "\n",
    "    ## 4. Select text column\n",
    "    if colname == \"title\":\n",
    "        df = df[['pmid', 'title', 'pubtype']]\n",
    "        df.rename({\"title\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"abs\":\n",
    "        df = df[['pmid', 'abstract', 'pubtype']]\n",
    "        df.rename({\"abstract\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"mix\":\n",
    "        df['mix'] = df[['title','abstract']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
    "        df = df[['pmid', 'mix', 'pubtype']]\n",
    "        df.rename({\"mix\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "\n",
    "    # Check the first few instances\n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head()) \n",
    "    \n",
    "    # No of lables and rows \n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)   \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "\n",
    "    ## 5. Split into X and y\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "     \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbiqsPHCUWVv"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_data_raw):\n",
    "    \"\"\"\n",
    "       Preprocess data with lowercase conversion, punctuation removal, tokenization, stemming\n",
    "       \n",
    "       X_data_raw: X data in dataframe\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    X_data=X_data_raw.iloc[:, -1].astype(str)\n",
    "   \n",
    "    # 1. convert all characters to lowercase\n",
    "    X_data = X_data.map(lambda x: x.lower())\n",
    "    \n",
    "    # 2. remove punctuation\n",
    "    X_data = X_data.str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    # 3. tokenize sentence\n",
    "    X_data = X_data.apply(nltk.word_tokenize)\n",
    "\n",
    "    # 4. remove stopwords\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    X_data = X_data.apply(lambda x: [word for word in x if word not in stopword_list])\n",
    "\n",
    "    #print(\"\\n<Data tokenzized and stopwords removed>\\n\", X_data)\n",
    "\n",
    "    # 5. lemmatize\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #X_data = X_data.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "    \n",
    "    # 5. stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    X_data = X_data.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    \n",
    "    # 6. removing unnecessary space\n",
    "    X_data = X_data.apply(lambda x: \" \".join(x)) \n",
    "    \n",
    "    print(\"\\n<After preprocessing training data>\")\n",
    "    print(X_data)\n",
    "    \n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFnFxlLdQc6V"
   },
   "outputs": [],
   "source": [
    "# plot loss and accuracy of training & validation\n",
    "def plot_history(history):\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    #acc = history.history['accuracy']\n",
    "    #val_acc = history.history['val_accuracy']\n",
    "    acc = history.history['binary_accuracy']\n",
    "    val_acc = history.history['val_binary_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    x = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsEIfStWuyEh"
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(maxlen, vocab_size, record):\n",
    "    \n",
    "    embedding_dim = 100\n",
    "  \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # adding embedding layer\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "\n",
    "    # adding a first convolutional layer\n",
    "    model.add(layers.Conv1D(512, 2, activation='relu'))\n",
    "  \n",
    "    # pooling layer\n",
    "    #model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.MaxPooling1D())\n",
    "\n",
    "    # adding a second convolutional layer with 512 filters\n",
    "    model.add(layers.Conv1D(512, 3, activation='relu'))\n",
    "\n",
    "    # second pooling layer\n",
    "    model.add(layers.MaxPooling1D())\n",
    "  \n",
    "    # flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # add dropout to prevent overfitting\n",
    "    model.add(layers.Dropout(0.5))\n",
    "  \n",
    "    # full connection\n",
    "    #model.add(layers.Dense(units=512))\n",
    "    #model.add(layers.Dense(units=1, activation='softmax'))  # for multi-classification\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "  \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['binary_accuracy',\n",
    "                           keras.metrics.Precision(name='precision'),\n",
    "                           keras.metrics.Recall(name='recall')])\n",
    "  \n",
    "    # summarize the model\n",
    "    print(\"\\n************* Model Summary *************\", file=record)\n",
    "    print(model.summary(), file=record)\n",
    "\n",
    "    print(\"\\n************* Model Summary *************\")\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJNK_S9QQc6X"
   },
   "source": [
    "# 3. Word Embedding: if want to use a precomputed embedding space: GloVE  \n",
    "\n",
    "File ('glove.6B.zip') is downloaded from https://nlp.stanford.edu/projects/glove/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKecLfQRQc6X"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):  \n",
    "    vocab_size = len(word_index) + 1  # adding 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix[idx] = np.array(\n",
    "                vector, dtype=np.float32)[:embedding_dim]\n",
    "                \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHfOFue0Qc6Y"
   },
   "outputs": [],
   "source": [
    "# after downloading embedding dict, put the file in the same directory as this notebook\n",
    "#filename = 'glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "def create_cnn_model_with_dic(filename, tokenizer, maxlen, vocab_size, record):\n",
    "    \n",
    "    embedding_dim = 100\n",
    "    \n",
    "    # To use embedding matrix to model\n",
    "    embedding_matrix = create_embedding_matrix(filename, tokenizer.word_index, embedding_dim)\n",
    "    \n",
    "    # how many nonzero embedding vectors\n",
    "    nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix))\n",
    "    print(nonzero_elements / vocab_size) \n",
    "  \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # use this to include embedding matrix\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                               output_dim=embedding_dim, \n",
    "                               weights=[embedding_matrix], \n",
    "                               input_length=maxlen, \n",
    "                               trainable=False))\n",
    "    \n",
    "    ## convolutional layers\n",
    "    # adding a first convolutional layer\n",
    "    model.add(layers.Conv1D(512, 2, activation='relu'))\n",
    "  \n",
    "    # pooling layer\n",
    "    #model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.MaxPooling1D())\n",
    "\n",
    "    # adding a second convolutional layer with 512 filters\n",
    "    model.add(layers.Conv1D(512, 3, activation='relu'))\n",
    "\n",
    "    # second pooling layer\n",
    "    model.add(layers.MaxPooling1D())\n",
    "  \n",
    "    # flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # add dropout to prevent overfitting\n",
    "    model.add(layers.Dropout(0.5))\n",
    "  \n",
    "    # full connection\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "  \n",
    "    ## compile the model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['binary_accuracy',\n",
    "                           keras.metrics.Precision(name='precision'),\n",
    "                           keras.metrics.Recall(name='recall')])\n",
    "  \n",
    "    ## summarize the model\n",
    "    print(\"\\n************* Model Summary *************\", file=record)\n",
    "    print(model.summary(), file=record)\n",
    "\n",
    "    print(\"\\n************* Model Summary *************\")\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5bxaV-8V7px"
   },
   "source": [
    "# 4. Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bp5AgJ1quDmW"
   },
   "outputs": [],
   "source": [
    "def main(input_file, \n",
    "         colname, \n",
    "         max_len, \n",
    "         batch_size,\n",
    "         epochs,\n",
    "         eval_on, \n",
    "         result_file,\n",
    "         datasize_change,\n",
    "         ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "       Main function for processing data, model training, and evaluation\n",
    "       \n",
    "       input_file: input file\n",
    "       colname: colume name for selection between title and abstract\n",
    "       max_len: max length of tokens\n",
    "       batch_size: batch size for traing model\n",
    "       epochs: number of training and validation loop\n",
    "       eval_on: indicator of model evaluation on or off\n",
    "       result_file: name of output file of evaluation\n",
    "       datasize_change: indicator of data size change on or off\n",
    "       ratio: proportion of data size\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    #### 0. open result file for records\n",
    "    f=open(result_file, \"a\")\n",
    "    \n",
    "    # Check the version of Tensorflow and Keras used\n",
    "    print(\"\\n************** Version **************\", file=f)\n",
    "    print(\"\\n************** Version **************\")\n",
    "    print(\"Tensorflow version: \", tensorflow.__version__, file=f)\n",
    "    print(\"Keras version: \", keras.__version__, file=f)\n",
    "    print(\"Tensorflow version: \", tensorflow.__version__)\n",
    "    print(\"Keras version: \", keras.__version__)\n",
    "    \n",
    "    # Check processing time\n",
    "    proc_start_time = timeit.default_timer()\n",
    "\n",
    "    #### 1. Load data \n",
    "    print(\"\\n************** Loading Data ************\\n\", file=f)\n",
    "    print(\"\\n************** Loading Data ************\\n\")\n",
    "    X, y = load_data_txt(input_file, colname, record=f)\n",
    "    \n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]), file=f)\n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]))\n",
    "    \n",
    "    #### 2. Train and test split\n",
    "    \n",
    "    print(\"\\n************** Spliting Data **************\\n\", file=f)\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test,y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "    \n",
    "    # For testing only: small size data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.01, random_state=42, stratify=y)\n",
    "    #X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=0.001, random_state=42, stratify=y_test)\n",
    "    #X_notuse, X_test, y_notuse, y_test = train_test_split(X_test, y_test, test_size=0.001, random_state=42, stratify=y_test)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(X_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape))\n",
    "    print(\"Val Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Val', file=f)\n",
    "    print(y_val.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "\n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(y_train.value_counts())\n",
    "    print('\\nClass Counts(label, row): Val')\n",
    "    print(y_val.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(y_test.value_counts())\n",
    "\n",
    "    print(\"\\n<X_train Data>\", file=f)\n",
    "    print(X_train.head(), file=f)\n",
    "    print(\"\\n<X_train Data>\")\n",
    "    print(X_train.head())\n",
    "\n",
    "    print(\"\\n<X_val Data>\", file=f)\n",
    "    print(X_val.head(), file=f)\n",
    "    print(\"\\n<X_val Data>\")\n",
    "    print(X_val.head())\n",
    "\n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "\n",
    "    #### 3. Data size change\n",
    "    \n",
    "    if datasize_change:\n",
    "        print(\"\\n************** Data Size Change *************\\n\", file=f)\n",
    "        print(\"Data Ratio (size): {} ({})\".format(ratio, int(X_train.shape[0]*ratio)), file=f)\n",
    "        print(\"\\n************** Data Size Change *************\\n\")\n",
    "        print(\"Data Size: {} ({})\".format(ratio, int(X_train.shape[0]*ratio)))\n",
    "        \n",
    "        X_train, _, y_train, _ = train_test_split(X_train, y_train, train_size=ratio, random_state=42, stratify=y_train)  \n",
    "    \n",
    "    # Reset index\n",
    "    X_train=X_train.reset_index(drop=True)\n",
    "    X_val=X_val.reset_index(drop=True)\n",
    "    X_test=X_test.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_val=y_val.reset_index(drop=True)\n",
    "    y_test=y_test.reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n************** Processing Data **************\", file=f)\n",
    "    print(\"\\n************** Processing Data **************\")\n",
    "    print(\"\\nTrain Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(X_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    print(\"\\nTrain Data: {}\".format(X_train.shape))\n",
    "    print(\"Val Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Val', file=f)\n",
    "    print(y_val.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "    print(\"\\n\", file=f)\n",
    "\n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(y_train.value_counts())\n",
    "    print('\\nClass Counts(label, row): Val')\n",
    "    print(y_val.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(y_test.value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"\\n<X_train Data>\", file=f)\n",
    "    print(X_train.head(), file=f)\n",
    "    print(\"\\n<X_train Data>\")\n",
    "    print(X_train.head())\n",
    "\n",
    "    print(\"\\n<X_val Data>\", file=f)\n",
    "    print(X_val.head(), file=f)\n",
    "    print(\"\\n<X_val Data>\")\n",
    "    print(X_val.head())\n",
    "\n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "    \n",
    "    ## 4. Transformation\n",
    "    print(\"\\n************** Transforming Text into Vectors **************\", file=f)\n",
    "    print(\"\\n************** Transforming Text into Vectors **************\")\n",
    "    sentences_train = X_train.iloc[:, -1]\n",
    "    sentences_val = X_val.iloc[:, -1]\n",
    "    sentences_test = X_test.iloc[:, -1]\n",
    "\n",
    "    print(\"\\nsentences_train: \", sentences_train.shape)\n",
    "    print(sentences_train.head())\n",
    "    print(\"\\nsentences_val: \", sentences_val.shape)\n",
    "    print(sentences_val.head())\n",
    "    print(\"\\nsentences_test: \", sentences_test.shape)\n",
    "    print(sentences_test.head())\n",
    "    \n",
    "    # prepare tokenizer\n",
    "    #tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    print(\"\\nvocab size: \", vocab_size, file=f)\n",
    "    print(\"\\nvocab size: \", vocab_size)\n",
    "\n",
    "    # integer encode the texts\n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    X_val = tokenizer.texts_to_sequences(sentences_val)\n",
    "    X_test = tokenizer.texts_to_sequences(sentences_test) \n",
    "\n",
    "    print(\"\\nFirst Instance: Train\\n\", sentences_train[0], file=f)\n",
    "    print(\"\\n\", X_train[0], file=f)\n",
    "    print(\"\\nFirst Instance: Val\\n\", sentences_val[0], file=f)\n",
    "    print(\"\\n\", X_val[0], file=f)\n",
    "    print(\"\\nFirst Instance: Test\\n\", sentences_test[0], file=f)\n",
    "    print(\"\\n\", X_test[0], file=f)\n",
    "\n",
    "    print(\"\\nFirst Instance: Train\\n\", sentences_train[0])\n",
    "    print(\"\\n\", X_train[0])\n",
    "    print(\"\\nFirst Instance: Val\\n\", sentences_val[0])\n",
    "    print(\"\\n\", X_val[0])\n",
    "    print(\"\\nFirst Instance: Test\\n\", sentences_test[0])\n",
    "    print(\"\\n\", X_test[0])\n",
    "    \n",
    "    # pad texts to a pre-defined max length\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "    X_val = pad_sequences(X_val, padding='post', maxlen=max_len)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "    \n",
    "    print(\"<X_train vector>\\n\", X_train[0, :], file=f)\n",
    "    print(\"<X_train vector>\\n\", X_train[0, :])\n",
    "\n",
    "    #### 5. Model Fitting\n",
    "    print(\"\\n************** Training Model: CNN **************\", file=f)\n",
    "    print(\"\\n************** Training Model: CNN **************\")\n",
    "\n",
    "    cnn_model = create_cnn_model(max_len, vocab_size, record=f)\n",
    "\n",
    "    history = cnn_model.fit(X_train, \n",
    "                            y_train, \n",
    "                            epochs=epochs,\n",
    "                            verbose=True,\n",
    "                            validation_data=(X_val, y_val),\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "    # plot loss & accuracy\n",
    "    print(\"\\n\")\n",
    "    plot_history(history)\n",
    "    \n",
    "    ## 6. Evaluating model performance\n",
    "    print('\\n************** Model Evaluation **************', file=f)\n",
    "    print('\\n************** Model Evaluation **************')\n",
    "\n",
    "    if eval_on:\n",
    "        loss, acc, pre, rec = cnn_model.evaluate(X_test, y_test, verbose=False)\n",
    "        f1 = 2 * ((pre*rec)/(pre+rec))\n",
    "\n",
    "        print(\"\\nTest evaluation: loss({:.4f}), acc({:.4f}), pre({:.4f}), rec({:.4f}))\".format(loss, acc, pre, rec), file=f)\n",
    "        print(\"\\nLoss: {:.4f}\".format(loss), file=f)\n",
    "        print(\"\\nAccuracy: {:.4f}\".format(acc), file=f)\n",
    "        print(\"\\nPrecision Recall F1\", file=f)\n",
    "        print(\"{:.4f}\\t{:.4f}\\t{:.4f}\".format(pre, rec, f1), file=f)\n",
    "\n",
    "        print(\"\\nTest evaluation: loss({:.4f}), acc({:.4f}), pre({:.4f}), rec({:.4f}))\".format(loss, acc, pre, rec))\n",
    "        print(\"\\nLoss: {:.4f}\".format(loss))\n",
    "        print(\"\\nAccuracy: {:.4f}\".format(acc))\n",
    "        print(\"\\nPrecision Recall F1\")\n",
    "        print(\"{:.4f}\\t{:.4f}\\t{:.4f}\".format(pre, rec, f1))\n",
    "\n",
    "    else:\n",
    "        print(\"No Evaluation Conducted\", file=f)\n",
    "        print(\"No Evaluation Conducted\")\n",
    "\n",
    "    # Create a classification report showing accuracy, precision, recall, f1\n",
    "    #predictions = cnn_model.predict(X_test)\n",
    "    #y_pred = np.argmax(predictions, axis=1)\n",
    "    predictions = cnn_model.predict(X_test)\n",
    "    y_pred = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "    print('\\nConfusion Matrix:', file=f)\n",
    "    print(confusion_matrix(y_test, y_pred), file=f)\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    print(\"\\n******** Classification Report ********\", file=f)\n",
    "    print(classification_report(y_test, y_pred, digits=4), file=f)\n",
    "    print(\"\\n******** Classification Report ********\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    print(\"\\nOutput file:'\" + result_file + \"' Created\", file=f)\n",
    "    print(\"\\nOutput file:'\" + result_file + \"' Created\")\n",
    "    \n",
    "    # check processing time\n",
    "    proc_elapsed = timeit.default_timer() - proc_start_time\n",
    "    print(\"\\nTotal Processing Time: {}min\\n\".format(round(proc_elapsed/60)), file=f)\n",
    "    print(\"\\nTotal Processing Time: {}min\\n\".format(round(proc_elapsed/60)))\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7BE9hI7r3ZU"
   },
   "source": [
    "# 5. Run code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAr3eaWgbkMr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "# \"%%time\" is commentized for running using linux terminal\n",
    "# If using a Jupyter notebook, uncommentize above code to check running time\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    \n",
    "    ######## 1. Set Parameter Values ########\n",
    "    \n",
    "    #### 1-1. Input file name & which column \n",
    "    input_filename=\"output_rct.txt\" \n",
    "    column_name = \"mix\"                                      # 'title' for title text; 'abs' for abstract; 'mix' for title + abstract\n",
    "    \n",
    "    #### 1-2. Data size change?\n",
    "    datachange_on=0                                            # 0 for no change; 1 for change of data size\n",
    "    ratio_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]   # ratio for data size\n",
    "\n",
    "    #### 1-3. Evaluating model performance?     \n",
    "    eval_on=1                                                  # 0 for no; 1 for yes (confusion matrix/classification report)\n",
    "    \n",
    "    #### 1-4. Hyperparameters for CNN\n",
    "    MAX_LEN = 512                                              # 150 for title; 512 for abs (Consistent with BERT parameters))\n",
    "    BATCH_SIZE = 16                                            # Batch size: 16 or 32\n",
    "    EPOCHS = 4                                                 # Number of epochs: 2,3,4\n",
    "\n",
    "    \n",
    "    ######## 2. Run Main Fuction ########\n",
    "    if datachange_on:               \n",
    "        for ratio in ratio_list: \n",
    "            eval_file = \"eval_cnn_\" + str(ratio) + \"_\" + column_name + \".txt\"\n",
    "            \n",
    "            main(input_file=input_filename,\n",
    "                 colname=column_name, \n",
    "                 max_len=MAX_LEN, \n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=EPOCHS,\n",
    "                 eval_on=eval_on, \n",
    "                 result_file=eval_file,\n",
    "                 datasize_change=datachange_on,\n",
    "                 ratio=ratio)\n",
    "    else:\n",
    "        eval_file = \"eval_cnn_all_\" + column_name + \".txt\" \n",
    "            \n",
    "        main(input_file=input_filename,\n",
    "             colname=column_name, \n",
    "             max_len=MAX_LEN, \n",
    "             batch_size=BATCH_SIZE,\n",
    "             epochs=EPOCHS,\n",
    "             eval_on=eval_on, \n",
    "             result_file=eval_file,\n",
    "             datasize_change=datachange_on,\n",
    "             ratio=1)\n",
    "        \n",
    "    print(\"\\n************** Processing Completed **************\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
