{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5iv-CjeceIf"
   },
   "source": [
    "# Text Classification using BERT with RCT data     \n",
    "\n",
    "This code is to build a pipeline to classify RCT (Randomized Controlled Trials) with Pubmed texts using pretrained deep learning models such as BERT, BioBERT, SciBERT.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttbMwR_7-oP6"
   },
   "source": [
    "Author: Jenna Kim  \n",
    "Created: 2021/1/11  \n",
    "Last Modified: 2021/10/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IDLjCo1-ZIz"
   },
   "source": [
    "## Update:  \n",
    "- Modify load_data funtion to read in txt file: V2  \n",
    "- Add code to remove duplicates: V2  \n",
    "- Add code to sample data for data size change: V2  \n",
    "- Modify code to sample data for label balance (1:1): V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d24Bh_zc6Fe"
   },
   "source": [
    "Reference:  \n",
    "* https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/ \n",
    "* https://www.youtube.com/watch?v=f-86-HcYYi8   \n",
    "* https://mccormickml.com/2019/07/22/BERT-fine-tuning/  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jdls3LTd2EV"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXZS0zvBe_-e"
   },
   "source": [
    "## 1-1. Install package and load libraires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9NHn7cleEe9"
   },
   "source": [
    "Install the transformers package from Hugging Face which is a pytorch interface for working with a BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vzsrzXe7c1Oy"
   },
   "outputs": [],
   "source": [
    "# Install transformer (ver 4.15.0)\n",
    "\n",
    "#!pip install transformers==4.15.0\n",
    "\n",
    "# No need to install PyTorch if this notebook is running on the AWS Sagemaker \n",
    "# with pytorch kernel('conda_pytorch_latest_p36')\n",
    "\n",
    "#!pip install torch==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Imbalanced-Learn library for sampling if not already installed\n",
    "\n",
    "#!pip install imbalanced-learn==0.8.1\n",
    "#!pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VGgZ9871oOL1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check if the packages are correctly installed\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvYcGEY5k0sE"
   },
   "source": [
    "Load other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V77AFJ2RjlJ2"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tMmmPU2Ik3m2"
   },
   "outputs": [],
   "source": [
    "# Set up for plots and paramters\n",
    "\n",
    "#%matplotlib inline\n",
    "#config InlineBackend.config_format='retina'\n",
    "\n",
    "sns.set(style='darkgrid', palette='muted', font_scale=1.5)\n",
    "COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(COLORS_PALETTE))\n",
    "rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NnWN2xAjr2XE"
   },
   "outputs": [],
   "source": [
    "# Hide warning messages from display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twm3tuiifF2y"
   },
   "source": [
    "## 1-2. Check GPU for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLbta3Vnf3P7"
   },
   "source": [
    "Note: If you use Google Colab, before running the next cell, make sure that the runtime type is set to GPU by going to Runtime => Change runtime type => GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTFWMkGPoOLz",
    "outputId": "661764ed-65a2-4653-fe02-f529d9218ac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\r\n",
      "Cuda compilation tools, release 10.1, V10.1.243\r\n"
     ]
    }
   ],
   "source": [
    "# Check a version of CUDA\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9-5qcHJeaMk",
    "outputId": "f502c0c6-1a19-4bc9-cdb7-2015daf35450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU:  Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# Check if there's a GPU available\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are {:d} GPU(s) available.'.format(torch.cuda.device_count()))\n",
    "    print('We will use the GPU: ', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6PuvWbAeTX",
    "outputId": "1758db2d-c68c-4712-fb72-80f695e34d25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-16GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7tcnC2SoOL3",
    "outputId": "baacdcc7-72f3-457c-c8f4-d25caeba2045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct  4 23:28:47 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    22W / 300W |      2MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# check GPU memory and & utilization\n",
    "!nvidia-smi\n",
    "\n",
    "# To check the GPU memory usage while the process is running\n",
    "# open a terminal in the directory (Go to New-> Terminal) and type the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yo6hOQqWoOL4"
   },
   "outputs": [],
   "source": [
    "# clear the occupied cuda memory for efficient use\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Kill a process in running if more GPU space is needed\n",
    "#!sudo kill -9 3320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5j__pEHeayH"
   },
   "source": [
    "# 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kiPOqDBoOL5"
   },
   "source": [
    "## 2-1. If you load data from Google Drive directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GCBTvaJueobV"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')\n",
    "#%cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXBsMyaGgshf"
   },
   "source": [
    "When running the above code, you might be required to enter authorization code to connect to Google Drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yb2IMEMneqD2"
   },
   "outputs": [],
   "source": [
    "# Access the directory where a dataset is stored\n",
    "#os.listdir(\"/gdrive/My Drive/Colab Notebooks/LabelingProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-UehQ6UihKnR"
   },
   "outputs": [],
   "source": [
    "#path = \"/gdrive/My Drive/Colab Notebooks/LabelingProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RckuoDMBhy9c"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "# IMDB dataset: similar to Medline data\n",
    "# Required for reducing the data size due to the GPU constraints \n",
    "#df_raw = pd.read_csv(os.path.join(path, \"IMDB Dataset.csv\"))  \n",
    "\n",
    "# CoLA dataset: one sentence per each instance\n",
    "#df1 = pd.read_csv(os.path.join(path, \"in_domain_train.tsv\"), delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "#df2 = pd.read_csv(os.path.join(path, \"in_domain_dev.tsv\"), delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "#print(df1.shape)\n",
    "#print(df2.shape)\n",
    "\n",
    "#df_raw = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#print(df_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmcQPrIBoOL7"
   },
   "source": [
    "## 2-2. If you load data from AWS SageMaker or your local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OB54atl5oOL7"
   },
   "outputs": [],
   "source": [
    "def load_data_txt(filename, colname, record):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file\n",
    "    record: text file to save summary\n",
    "\n",
    "    return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    ## 1. Read in data from input file\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", encoding='utf-8', header=None, names=['pmid', 'pubtype', 'year', 'title', 'abstract'])\n",
    "    \n",
    "    # No of rows and columns\n",
    "    print(\"No of Rows (Raw data): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record)\n",
    "    print(\"No of Rows (Raw data): {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "    \n",
    "    ## 2. Select data needed for processing & convert labels\n",
    "    df = df[['pmid', 'title', 'abstract', 'pubtype']]\n",
    "    \n",
    "    ## 3. Cleaning data \n",
    "    #Trim unnecessary spaces for strings\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: x.strip())\n",
    "    df[\"abstract\"] = df[\"abstract\"].apply(lambda x: x.strip())\n",
    "\n",
    "    # Remove null values \n",
    "    df=df.dropna()\n",
    "\n",
    "    print(\"No of rows (After dropping null): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record)\n",
    "    print(\"No of rows (After dropping null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    # Remove duplicates and keep first occurrence\n",
    "    df.drop_duplicates(subset=['pmid'], keep='first', inplace=True)\n",
    "\n",
    "    print(\"No of rows (After removing duplicates): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of rows (After removing duplicates): {}\".format(df.shape[0]))\n",
    "        \n",
    "    ## 4. Select text columns\n",
    "    if colname == \"title\":\n",
    "        df = df[['pmid', 'title', 'pubtype']]\n",
    "        df.rename({\"title\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"abs\":\n",
    "        df = df[['pmid', 'abstract', 'pubtype']]\n",
    "        df.rename({\"abstract\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"mix\":\n",
    "        df['mix'] = df[['title','abstract']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
    "        df = df[['pmid', 'mix', 'pubtype']]\n",
    "        df.rename({\"mix\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "\n",
    "    # Check the first few instances\n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head())\n",
    "    \n",
    "    # No of lables and rows \n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)   \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-g5NKtYG86kX"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, colname, record):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file\n",
    "    record: text file to save summary\n",
    "\n",
    "    return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(filename, encoding='utf-8')\n",
    "    \n",
    "    # No of rows and columns\n",
    "    print(\"No of Rows (Raw data): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    print(\"No of Rows (Raw data): {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "    \n",
    "    # Select data needed for processing & convert labels\n",
    "    df = df[['pmid', 'title', 'abstract', 'pubtype']]\n",
    "    df.iloc[:, -1] = df.iloc[:, -1].map({'RCT':1, 'Other':0})\n",
    "    \n",
    "    # Remove null values \n",
    "    df=df.dropna()\n",
    "\n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "        \n",
    "    # Select text columns\n",
    "    if colname == \"title\":\n",
    "        df = df[['pmid', 'title', 'pubtype']]\n",
    "        df.rename({\"title\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"abs\":\n",
    "        df = df[['pmid', 'abstract', 'pubtype']]\n",
    "        df.rename({\"abstract\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"mix\":\n",
    "        df['mix'] = df[['title','abstract']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
    "        df = df[['pmid', 'mix', 'pubtype']]\n",
    "        df.rename({\"mix\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "\n",
    "    # Check the first few instances\n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head())\n",
    "    \n",
    "    # No of lables and rows \n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVTACKnXqXxM"
   },
   "source": [
    "# 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzFLjPm9vVpp"
   },
   "source": [
    "## 3-1. Check the distribution of token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "G8CCdCXooOL8"
   },
   "outputs": [],
   "source": [
    "def token_distribution(df, tokenizer):\n",
    "    token_lens = []\n",
    "    long_tokens = []\n",
    "    \n",
    "    for pmid, txt in zip(df.pmid, df.sentence):\n",
    "        tokens = tokenizer.encode(txt, padding=True, truncation=True, max_length=512)\n",
    "        token_lens.append(len(tokens))\n",
    "    \n",
    "        # Check a sentence with extreme length\n",
    "        if len(tokens) > 150:\n",
    "            long_tokens.append((pmid, len(tokens)))   \n",
    "  \n",
    "    print(\"\\nLong Sentences: \")\n",
    "\n",
    "    if len(long_tokens)>0:\n",
    "      print(long_tokens) \n",
    "    else:\n",
    "      print(\"There is no long sentence\")\n",
    "    \n",
    "    print(\"\\nMin token:\", min(token_lens))\n",
    "    print(\"Max token:\", max(token_lens))\n",
    "    print(\"Avg token:\", round(sum(token_lens)/len(token_lens)))\n",
    "    \n",
    "    # plot the distribution\n",
    "    #sns.displot(token_lens)\n",
    "    #plt.xlim([0, max(token_lens)+10])\n",
    "    #plt.xlabel(\"Token Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqBtWF8Mw3Hx"
   },
   "source": [
    "## 3-2. Create a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t5_qP5fQw7p5"
   },
   "outputs": [],
   "source": [
    "class LabelDataset(Dataset):\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        review = \" \".join(review.split())\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,                    # second parameter is needed for a task of sentence similarity\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,  \n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),            # flatten() reduce dimension: e.g., [1, 512] -> [512]\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A18krOXzNar"
   },
   "source": [
    "## 3-3. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mDmZE8qYoOL8"
   },
   "outputs": [],
   "source": [
    "def sample_data(X_train, y_train, record, sampling=0, sample_method='over'):\n",
    "    \"\"\"\n",
    "       Sampling input train data\n",
    "       \n",
    "       X_train: dataframe of X train data\n",
    "       y_train: datafram of y train data\n",
    "       sampling: indicator of sampling funtion is on or off\n",
    "       sample_method: method of sampling (oversampling or undersampling)\n",
    "       record: text file to save summary\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    if sampling:\n",
    "        # select a sampling method\n",
    "        if sample_method == 'over':\n",
    "            oversample = RandomOverSampler(random_state=42)\n",
    "            X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******', file=record)\n",
    "            print('\\n****** Data Sampling ******')\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()), file=record)\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_over, y_over\n",
    "            \n",
    "        elif sample_method == 'under':\n",
    "            undersample = RandomUnderSampler(random_state=42)\n",
    "            X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******', file=record)\n",
    "            print('\\n****** Data Sampling ******')\n",
    "            print('\\nUndersampled Data (class,Rows):\\n{}'.format(y_under.value_counts()), file=record)\n",
    "            print('\\nUndersampled Data (class,Rows):\\n{}'.format(y_under.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_under, y_under\n",
    "    else:\n",
    "        X_train_sam, y_train_sam = X_train, y_train \n",
    "        print('\\n****** Data Sampling ******', file=record)\n",
    "        print('\\n****** Data Sampling ******')\n",
    "        print('\\nNo Sampling Performed\\n', file=record)\n",
    "        print('\\nNo Sampling Performed\\n')\n",
    "    \n",
    "    return X_train_sam, y_train_sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7Cqg09z2cOM"
   },
   "source": [
    "## 3-4. Create a data loader & classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FKtOnpVT12oS"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = LabelDataset(\n",
    "        reviews = df.sentence.to_numpy(),\n",
    "        targets = df.label.to_numpy(),\n",
    "        tokenizer = tokenizer,\n",
    "        max_len = max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RbwJrTtJSH9m"
   },
   "outputs": [],
   "source": [
    "class LabelClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, pretrained_model):\n",
    "        super(LabelClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_out = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids)\n",
    "        output_dropout = self.dropout(bert_out.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-pK1UKzVEtx"
   },
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5MrPe8HdXup"
   },
   "source": [
    "## 4-1. Hyperparameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAi3CD_oeCkg"
   },
   "source": [
    "The BERT authors's recommendations for fine-tuning:  \n",
    "* Batch size: 16, 32  \n",
    "* Learning rate (Adam): 5e-5, 3e-5, 2e-5  \n",
    "* Number of epochs: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nTHCPOsheA30"
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    n_examples,\n",
    "    outfile):\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # printout for checking the prediction & target\n",
    "        #print(\"Pred: \", preds)\n",
    "        #print(\"Target: \", targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(\"Correct Prediction (Train): {} out of {}\".format(correct_predictions.int(), n_examples), file=outfile)\n",
    "    print(\"Correct Prediction (Train): {} out of {}\".format(correct_predictions.int(), n_examples))\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mkBOEEtbhofC"
   },
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    model, \n",
    "    data_loader, \n",
    "    loss_fn, \n",
    "    device, \n",
    "    n_examples,\n",
    "    outfile):\n",
    "    \n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids\n",
    "                )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    print(\"Correct Prediction (Eval): {} out of {}\".format(correct_predictions.int(), n_examples), file=outfile)\n",
    "    print(\"Correct Prediction (Eval): {} out of {}\".format(correct_predictions.int(), n_examples))\n",
    "    \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xHJomabfoOL-"
   },
   "outputs": [],
   "source": [
    "def plot_train_history(history):\n",
    "    plt.plot(history[\"train_acc\"], 'b-o', label=\"train accuracy\")\n",
    "    plt.plot(history[\"val_acc\"], 'r-o', label=\"validation accuracy\")\n",
    "\n",
    "    plt.title(\"Training History\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.xticks(history[\"epoch\"])\n",
    "    plt.yticks(np.arange(0,1.2,step=0.05))\n",
    "    plt.ylim([0,1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nvYnWEDsjUKE"
   },
   "outputs": [],
   "source": [
    "def training_loop(epochs, \n",
    "                  modelname, \n",
    "                  model, \n",
    "                  train_data_loader, \n",
    "                  val_data_loader, \n",
    "                  loss_fn, \n",
    "                  optimizer, \n",
    "                  device, \n",
    "                  scheduler, \n",
    "                  n_train, \n",
    "                  n_val,\n",
    "                  model_file,\n",
    "                  record):\n",
    "    \n",
    "    print(\"\\n**** Model Name: \" + modelname + \" *****\", file=record)\n",
    "    print(\"\\n**** Model Name: \" + modelname + \" *****\")\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nEpoch {} / {}\".format(str(epoch + 1), str(epochs)), file=record)\n",
    "        print(\"-\" * 60, file=record)\n",
    "    \n",
    "        print(\"\\nEpoch {} / {}\".format(str(epoch + 1), str(epochs)))\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "        train_acc, train_loss = train_model(\n",
    "            model, \n",
    "            train_data_loader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            n_train,\n",
    "            outfile=record)\n",
    "    \n",
    "        print(\"Train Loss: {}, Accuracy: {}\\n\".format(train_loss, train_acc), file=record)\n",
    "        print(\"Train Loss: {}, Accuracy: {}\\n\".format(train_loss, train_acc))\n",
    "    \n",
    "        val_acc, val_loss = eval_model(\n",
    "            model,\n",
    "            val_data_loader,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            n_val,\n",
    "            outfile=record)\n",
    "    \n",
    "        print(\"Validation Loss: {}, Accuracy: {}\".format(val_loss, val_acc), file=record)  \n",
    "        print(\"Validation Loss: {}, Accuracy: {}\".format(val_loss, val_acc))\n",
    "\n",
    "        # store the state of the best model using the higest validation accuracy\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            if model_file:\n",
    "                torch.save(model.state_dict(), model_file)\n",
    "            best_accuracy = val_acc\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    #plot_train_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERH77nypnuFa"
   },
   "source": [
    "# 5. Predictions & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_iJYc8yzn02l"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Apply the softmax or sigmoid function to normalize the raw output(logits) to get probability for each clas\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    # move the data to cpu\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XAxJl4eFoOL_"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred, record, eval_model=0):\n",
    "    \"\"\"\n",
    "      evaluate model performance\n",
    "      \n",
    "      y_test: y test data\n",
    "      y_pred: t prediction score\n",
    "      eval_model: indicator if this funtion is on or off\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    if eval_model:\n",
    "        \n",
    "        print('\\n************** Model Evaluation **************', file=record)\n",
    "        print('\\n************** Model Evaluation **************')\n",
    "        \n",
    "        print('\\nConfusion Matrix:\\n', file=record)\n",
    "        print('\\nConfusion Matrix:\\n')\n",
    "        print(confusion_matrix(y_test, y_pred), file=record)\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        print('\\nClassification Report:\\n', file=record)\n",
    "        print('\\nClassification Report:\\n')\n",
    "        print(classification_report(y_test, y_pred, digits=4), file=record)\n",
    "        print(classification_report(y_test, y_pred, digits=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TQD2oIhdoOL_"
   },
   "outputs": [],
   "source": [
    "def predict_proba(df_test, y_text, y_test, y_pred, y_pred_probs, proba_file, proba_out=0):\n",
    "    \n",
    "    \"\"\"\n",
    "       Predict probability of each class\n",
    "       \n",
    "       df_test: original X test data\n",
    "       y_text: text data sentence\n",
    "       y_test: original y test data\n",
    "       y_pred: predicted y values\n",
    "       y_pred_probs: probability scores of prediction\n",
    "       proba_file: output file of probability scores\n",
    "       proba_on: decide if the probability output is expected\n",
    "       \n",
    "    \"\"\"\n",
    "    if proba_out:\n",
    "        df_result = pd.DataFrame({\n",
    "            'pmid': df_test[\"pmid\"],\n",
    "            'text': y_text,\n",
    "            'act': y_test,\n",
    "            'pred': y_pred,\n",
    "            'proba_0': y_pred_probs[:, 0],\n",
    "            'prob_1': y_pred_probs[:, 1]\n",
    "        })\n",
    "        \n",
    "        ## Save output\n",
    "        df_result.to_csv(proba_file, encoding='utf-8', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ZwBgE5oOL_"
   },
   "source": [
    "# 6. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vZnWb_HaoOL_"
   },
   "outputs": [],
   "source": [
    "def main(input_file, \n",
    "         colname, \n",
    "         sample_on, \n",
    "         sample_type, \n",
    "         tokenizer,\n",
    "         max_len, \n",
    "         batch_size,\n",
    "         modelname,\n",
    "         n_class,\n",
    "         device,\n",
    "         pretrained_model,\n",
    "         learning_rate,\n",
    "         epochs,\n",
    "         model_file, \n",
    "         eval_on, \n",
    "         proba_on,\n",
    "         proba_file,\n",
    "         result_file,\n",
    "         datasize_change,\n",
    "         sample_balance,\n",
    "         balance_sampling_on,                                   \n",
    "         balance_sampling_type,\n",
    "         sample_ratio,\n",
    "         ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "       Main function for processing data, model training, and prediction\n",
    "       \n",
    "       input_file: input file\n",
    "       colname: colume name for selection between title and abstract\n",
    "       sample_on: indicator of sampling on or off\n",
    "       sample_type: sample type to choose if sample_on is 1\n",
    "       model_method: name of classifier to be applied for model fitting\n",
    "       eval_on: indicator of model evaluation on or off\n",
    "       proba_file: name of output file of probability\n",
    "       result_file: name of output file of evaluation\n",
    "       ratio: proportion of data size\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    ## 0. open result file for records\n",
    "    f=open(result_file, \"a\")\n",
    "    \n",
    "    # Check processing time\n",
    "    proc_start_time = timeit.default_timer()\n",
    "    \n",
    "    ## 1. Load data\n",
    "    \n",
    "    print(\"\\n************** Loading Data **************\\n\", file=f)\n",
    "    print(\"\\n************** Loading Data **************\\n\")\n",
    "    #df = load_data(input_file, colname, record=f)        # use for comma-delimited csv file\n",
    "    df = load_data_txt(input_file, colname, record=f)     # use for tab-delimited txt file\n",
    "    \n",
    "    # testing\n",
    "    print(\"\\nFirst Sentence: \", df.sentence[0], file=f)\n",
    "    print(\"\\nFirst Sentence: \", df.sentence[0])\n",
    "\n",
    "    ## 2. Train and test split\n",
    "    \n",
    "    print(\"\\n************** Spliting Data **************\\n\", file=f)\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "    \n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df.label)\n",
    "    df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42, stratify=df_test.label)\n",
    "    \n",
    "    #for testing only: small size data\n",
    "    #df_train, df_test = train_test_split(df, test_size=0.99, random_state=42, stratify=df.label)\n",
    "    #df_val, df_test = train_test_split(df_test, test_size=0.99, random_state=42, stratify=df_test.label)\n",
    "    #df_notuse, df_test = train_test_split(df_test, test_size=0.01, random_state=42, stratify=df_test.label)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(df_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(df_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(df_test.shape), file=f)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(df_train.shape))\n",
    "    print(\"Val Data: {}\".format(df_val.shape))\n",
    "    print(\"Test Data: {}\".format(df_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(df_train.label.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Val', file=f)\n",
    "    print(df_val.label.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(df_test.label.value_counts(), file=f)\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(df_train.label.value_counts())\n",
    "    print('\\nClass Counts(label, row): Val')\n",
    "    print(df_val.label.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(df_test.label.value_counts())\n",
    "    \n",
    "    print(\"\\nTest Data\", file=f)\n",
    "    print(df_test.head(), file=f)\n",
    "    print(\"\\nTest Data\")\n",
    "    print(df_test.head())\n",
    "    \n",
    "    ## 3. Data size change\n",
    "    \n",
    "    if datasize_change:\n",
    "        \n",
    "        # Sample size reduce: 500,000 instance -> 100,000 instance\n",
    "        df_train, _ = train_test_split(df_train, train_size=0.2, random_state=42, stratify=df_train.label)\n",
    "        df_val, _ = train_test_split(df_val, train_size=0.2, random_state=42, stratify=df_val.label)\n",
    "        df_test, _ = train_test_split(df_test, train_size=0.2, random_state=42, stratify=df_test.label)\n",
    "        \n",
    "        print(\"\\n************** Data Size Change: Reducing Data **************\\n\", file=f)\n",
    "        print(\"\\n************** Data Size Change: Reducing Data **************\\n\")\n",
    "        print(\"Train Data: {}\".format(df_train.shape), file=f)\n",
    "        print(\"Val Data: {}\".format(df_val.shape), file=f)\n",
    "        print(\"Test Data: {}\".format(df_test.shape), file=f)\n",
    "        print(\"Train Data: {}\".format(df_train.shape))\n",
    "        print(\"Val Data: {}\".format(df_val.shape))\n",
    "        print(\"Test Data: {}\".format(df_test.shape))\n",
    "        \n",
    "        print('\\nClass Counts(label, row): Train', file=f)\n",
    "        print(df_train.label.value_counts(), file=f)\n",
    "        print('\\nClass Counts(label, row): Val', file=f)\n",
    "        print(df_val.label.value_counts(), file=f)\n",
    "        print('\\nClass Counts(label, row): Test', file=f)\n",
    "        print(df_test.label.value_counts(), file=f)\n",
    "        print(\"\\n\", file=f)\n",
    "    \n",
    "        print('\\nClass Counts(label, row): Train')\n",
    "        print(df_train.label.value_counts())\n",
    "        print('\\nClass Counts(label, row): Val')\n",
    "        print(df_val.label.value_counts())\n",
    "        print('\\nClass Counts(label, row): Test')\n",
    "        print(df_test.label.value_counts())\n",
    "        \n",
    "        print(\"\\n<Train Data>\", file=f)\n",
    "        print(df_train.head(), file=f)\n",
    "        print(\"\\n<Train Data>\")\n",
    "        print(df_train.head())\n",
    "    \n",
    "        print(\"\\nTest Data\", file=f)\n",
    "        print(df_test.head(), file=f)\n",
    "        print(\"\\nTest Data\")\n",
    "        print(df_test.head())\n",
    "        \n",
    "        # Sample data with balance (1:1)\n",
    "        if sample_balance:\n",
    "            \n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\", file=f)\n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\")\n",
    "            \n",
    "            # split into X and y\n",
    "            X_train, y_train = df_train.iloc[:, :-1], df_train.iloc[:, -1]\n",
    "            \n",
    "            # sampling\n",
    "            X_train, y_train = sample_data(X_train, y_train, record=f, \n",
    "                                           sampling=balance_sampling_on, \n",
    "                                           sample_method=balance_sampling_type)\n",
    "            \n",
    "            \n",
    "            print('\\nClass Counts(label, row): After balancing', file=f)\n",
    "            print(y_train.value_counts(), file=f)\n",
    "            print('\\nClass Counts(label, row): After balancing')\n",
    "            print(y_train.value_counts())\n",
    "            print(\"\\n<Balanced Train Data>\", file=f)\n",
    "            print(X_train.head(), file=f)\n",
    "            print(\"\\n<Balanced Train Data>\")\n",
    "            print(X_train.head()) \n",
    "            \n",
    "            # merge into one dataframe\n",
    "            df_train = pd.concat([X_train, y_train], axis=1)\n",
    "            \n",
    "        # Sample data based on size ratio    \n",
    "        if sample_ratio:\n",
    "            if ratio == 1:\n",
    "                df_train = df_train         \n",
    "            else:              \n",
    "                df_train, _ = train_test_split(df_train, train_size=ratio, random_state=42, stratify=df_train.label)\n",
    "                \n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\", file=f)\n",
    "            print(\"Data Ratio: {}\".format(ratio), file=f)\n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\")\n",
    "            print(\"Data Ratio: {}\".format(ratio))\n",
    "            \n",
    "            print('\\nClass Counts(label, row): After sampling', file=f)\n",
    "            print(df_train.label.value_counts(), file=f)\n",
    "            print('\\nClass Counts(label, row): After sampling')\n",
    "            print(df_train.label.value_counts())\n",
    "            print(\"\\n<Train Data Based on Ratio>\", file=f)\n",
    "            print(df_train.head(), file=f)\n",
    "            print(\"\\n<Train Data Based on Ratio>\")\n",
    "            print(df_train.head()) \n",
    "    \n",
    "    # Reset index\n",
    "    df_train=df_train.reset_index(drop=True)\n",
    "    df_val=df_val.reset_index(drop=True)\n",
    "    df_test=df_test.reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n************** Processing Data **************\", file=f)\n",
    "    print(\"\\n************** Processing Data **************\")\n",
    "    print(\"Train Data: {}\".format(df_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(df_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(df_test.shape), file=f)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(df_train.shape))\n",
    "    print(\"Val Data: {}\".format(df_val.shape))\n",
    "    print(\"Test Data: {}\".format(df_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(df_train.label.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Val', file=f)\n",
    "    print(df_val.label.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(df_test.label.value_counts(), file=f)\n",
    "    print(\"\\n\", file=f)\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(df_train.label.value_counts())\n",
    "    print('\\nClass Counts(label, row): Val')\n",
    "    print(df_val.label.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(df_test.label.value_counts())\n",
    "    \n",
    "    print(\"\\nTest Data\", file=f)\n",
    "    print(df_test.head(), file=f)\n",
    "    print(\"\\nTest Data\")\n",
    "    print(df_test.head())\n",
    "    \n",
    "    ## 4. Sampling\n",
    "    if sample_on:\n",
    "        X_train = df_train.iloc[:, :-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "    \n",
    "        # Sampling\n",
    "        X_train_samp, y_train_samp = sample_data(X_train, y_train, sampling=sample_on, sample_method=sample_type)\n",
    "    \n",
    "        print(y_train_samp.value_counts(), file=f)\n",
    "\n",
    "        # Combine x_train and y_train data\n",
    "        df_train_concat = pd.concat([X_train_samp, y_train_samp], axis=1)\n",
    "\n",
    "        print(df_train_concat.info())\n",
    "        print(df_train_concat.head())\n",
    "    \n",
    "        # replace train data with sampled data\n",
    "        df_train = df_train_concat\n",
    "        print(df_train.shape)\n",
    "    \n",
    "    ## 5. Load data\n",
    "    train_data_loader = create_data_loader(df_train, tokenizer, max_len, batch_size)\n",
    "    val_data_loader = create_data_loader(df_val, tokenizer, max_len, batch_size)\n",
    "    test_data_loader = create_data_loader(df_test, tokenizer, max_len, batch_size)\n",
    "\n",
    "    ## 6. Model Training\n",
    "    print(\"\\n************** Training Model: \" + modelname + \" **************\", file=f)\n",
    "    print(\"\\n************** Training Model: \" + modelname + \" **************\")\n",
    "    \n",
    "    # Check training time\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    n_train = len(df_train)    \n",
    "    n_val = len(df_val)\n",
    "    \n",
    "    # Create a classifier instance and move it to GPU\n",
    "    model = LabelClassifier(n_class, pretrained_model)\n",
    "    model = model.to(device)   \n",
    "    \n",
    "    # Optimizer, scheduler, loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False)\n",
    "    total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = 0,\n",
    "        num_training_steps = total_steps)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Loop training with epochs\n",
    "    training_loop(epochs, modelname, model, \n",
    "                train_data_loader, val_data_loader, \n",
    "                loss_fn, optimizer, device, scheduler, \n",
    "                n_train, n_val, model_file=None, record=f)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"\\nTraining Time ({} epochs): {}\".format(epochs, round(elapsed, 2)), file=f)\n",
    "    print(\"\\nTraining Time ({} epochs): {}\".format(epochs, round(elapsed,2)))\n",
    "    \n",
    "    ## 7. Prediction   \n",
    "    print(\"\\n\\n************** Getting predictions **************\", file=f)\n",
    "    print(\"\\n\\n************** Getting predictions **************\")\n",
    "    y_text, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader)  \n",
    "    \n",
    "    ## 8. Evaluating model performance      \n",
    "    print(\"\\n************** Evaluating performance **************\", file=f)\n",
    "    print(\"\\n************** Evaluating performance **************\")\n",
    "    evaluate_model(y_test, y_pred, record=f, eval_model=eval_on)\n",
    "    \n",
    "    ## 9. Probability prediction    \n",
    "    predict_proba(df_test, y_text, y_test, y_pred, y_pred_probs, proba_file=proba_file, proba_out=proba_on)\n",
    "    \n",
    "    print(\"\\nOutput file: '\" + result_file + \"' Created\", file=f)\n",
    "    print(\"\\nOutput file: '\" + result_file + \"' Created\")\n",
    "    \n",
    "    proc_elapsed = timeit.default_timer() - proc_start_time\n",
    "    print(\"\\nTotal Processing Time: {}min\\n\".format(round(proc_elapsed/60)), file=f)\n",
    "    print(\"\\nTotal Processing Time: {}min\\n\".format(round(proc_elapsed/60)))\n",
    "    \n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbPpOVyVoOMA"
   },
   "source": [
    "# 7. Run Code for Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1RxI-Z6FoOMA",
    "outputId": "b8a36b12-5422-4ef6-d1d8-50a03ca45e41",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Loading Data **************\n",
      "\n",
      "No of Rows (Raw data): 500068\n",
      "No of Columns: 5\n",
      "No of rows (After dropping null): 500068\n",
      "No of columns: 4\n",
      "No of rows (After removing duplicates): 499963\n",
      "\n",
      "<Data View: First Few Instances>\n",
      "\n",
      "        pmid                                           sentence  label\n",
      "0  18439781  In the United States, an increasing number of ...      0\n",
      "1  18468833  The American Heart Association website defines...      0\n",
      "2  18481181  The complex pathophysiology of traumatic brain...      0\n",
      "3  18728056  [BACKGROUND] Soluble CD40 ligand (sCD40L) is a...      1\n",
      "4  18790590  [BACKGROUND] Internal carotid artery dissectio...      0\n",
      "\n",
      "Class Counts(label, row): Total\n",
      "0    399977\n",
      "1     99986\n",
      "Name: label, dtype: int64\n",
      "\n",
      "First Sentence:  In the United States, an increasing number of law enforcement agencies have employed the use of TASER® (TASER International Inc., Scottsdale, AZ) devices to temporarily immobilize violent subjects. There are reports in the lay press of adverse outcomes occurring in patients on whom TASER® devices have been deployed. Rhabdomyolysis has been associated with patients sustaining a TASER® shock, with a 1% incidence rate in subjects subdued with earlier versions of the device and then brought to the Emergency Department (ED). We present the cases of 2 patients who were seen in our ED after exhibiting violent behavior and receiving TASER® shocks. Both were hospitalized and received treatment for mild rhabdomyolysis. Both patients had multiple other characteristics that have been found to have an association with the development of rhabdomyolysis, in addition to the shocks they received. A review and discussion of the available medical literature on the subject follows, describing several complications that have been documented in patients after receiving TASER® shocks. Although a direct link between the TASER® and the reported adverse effects has not been established, patients who undergo restraint via this device frequently have pre-existing conditions or have exhibited behavior that places them at risk for the development of those effects. Such awareness of these possible complications is vital because the evaluation and management of patients developing adverse effects after these events will commonly occur in the ED.\n",
      "\n",
      "************** Spliting Data **************\n",
      "\n",
      "Train Data: (399970, 3)\n",
      "Val Data: (49996, 3)\n",
      "Test Data: (49997, 3)\n",
      "\n",
      "Class Counts(label, row): Train\n",
      "0    319981\n",
      "1     79989\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Val\n",
      "0    39998\n",
      "1     9998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Test\n",
      "0    39998\n",
      "1     9999\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test Data\n",
      "            pmid                                           sentence  label\n",
      "141906  24081098  We explain a technique that recovers the struc...      0\n",
      "5372    21075537  Post-burn pruritis is a very distressing sympt...      1\n",
      "251994  26642726  Auditory or visual hallucinations of a decease...      0\n",
      "10967   21255258  [PURPOSE] The study aims to assess indicators ...      0\n",
      "266669  26996521  Mid-Late Holocene environmental changes in the...      0\n",
      "\n",
      "************** Data Size Change: Reducing Data **************\n",
      "\n",
      "Train Data: (79994, 3)\n",
      "Val Data: (9999, 3)\n",
      "Test Data: (9999, 3)\n",
      "\n",
      "Class Counts(label, row): Train\n",
      "0    63996\n",
      "1    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Val\n",
      "0    7999\n",
      "1    2000\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Test\n",
      "0    7999\n",
      "1    2000\n",
      "Name: label, dtype: int64\n",
      "\n",
      "<Train Data>\n",
      "            pmid                                           sentence  label\n",
      "132892  23883649  [INTRODUCTION] The objective of this article w...      0\n",
      "432914  31293160  Inhibiting starch digestion can effectively co...      0\n",
      "202418  25490610  [BACKGROUND] The prevalence of hepatitis-C-vir...      1\n",
      "465499  32186175  Plastic materials contain various additives, w...      0\n",
      "494695  33218043  Tobacco (Nicotiana tabacum) pollen is a well-s...      0\n",
      "\n",
      "Test Data\n",
      "            pmid                                           sentence  label\n",
      "339817  28895087  [BACKGROUND] This study aimed to quantify the ...      0\n",
      "445141  31610995  [OBJECTIVES] The benefit of alerting clinical ...      1\n",
      "37112   21824051  We evaluated sequential bortezomib, liposomal ...      0\n",
      "271607  27115241  [BACKGROUND] Hearing loss and dementia are bot...      1\n",
      "287114  27526991  [OBJECTIVE] Our study objective was to determi...      0\n",
      "\n",
      "************** Data Balancing: Label Class (1:1) *************\n",
      "\n",
      "\n",
      "****** Data Sampling ******\n",
      "\n",
      "Undersampled Data (class,Rows):\n",
      "1    15998\n",
      "0    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): After balancing\n",
      "1    15998\n",
      "0    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "<Balanced Train Data>\n",
      "       pmid                                           sentence\n",
      "0  26304469  Esophageal diverticula are rare findings that ...\n",
      "1  23378388  The patient was a 63-year-old female who had a...\n",
      "2  31246674  [BACKGROUND] The purpose of this article is to...\n",
      "3  28108738  Owing to the central role of apoptosis in many...\n",
      "4  25713042  A Gram-stain-negative, non-motile, aerobic and...\n",
      "\n",
      "************** Processing Data **************\n",
      "Train Data: (31996, 3)\n",
      "Val Data: (9999, 3)\n",
      "Test Data: (9999, 3)\n",
      "\n",
      "Class Counts(label, row): Train\n",
      "1    15998\n",
      "0    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Val\n",
      "0    7999\n",
      "1    2000\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Test\n",
      "0    7999\n",
      "1    2000\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test Data\n",
      "       pmid                                           sentence  label\n",
      "0  28895087  [BACKGROUND] This study aimed to quantify the ...      0\n",
      "1  31610995  [OBJECTIVES] The benefit of alerting clinical ...      1\n",
      "2  21824051  We evaluated sequential bortezomib, liposomal ...      0\n",
      "3  27115241  [BACKGROUND] Hearing loss and dementia are bot...      1\n",
      "4  27526991  [OBJECTIVE] Our study objective was to determi...      0\n",
      "\n",
      "************** Training Model: scibert_scivocab_cased **************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06c5580ccb9435e9fff0293f1bb062f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Model Name: scibert_scivocab_cased *****\n",
      "\n",
      "Epoch 1 / 4\n",
      "------------------------------------------------------------\n",
      "Correct Prediction (Train): 31914 out of 31996\n",
      "Train Loss: 0.022651843547821043, Accuracy: 0.997437179647456\n",
      "\n",
      "Correct Prediction (Eval): 2000 out of 9999\n",
      "Validation Loss: 9.38027992477417, Accuracy: 0.20002000200020004\n",
      "\n",
      "Epoch 2 / 4\n",
      "------------------------------------------------------------\n",
      "Correct Prediction (Train): 31850 out of 31996\n",
      "Train Loss: 0.039395029794424774, Accuracy: 0.9954369296162021\n",
      "\n",
      "Correct Prediction (Eval): 2000 out of 9999\n",
      "Validation Loss: 8.904176417541503, Accuracy: 0.20002000200020004\n",
      "\n",
      "Epoch 3 / 4\n",
      "------------------------------------------------------------\n",
      "Correct Prediction (Train): 31789 out of 31996\n",
      "Train Loss: 0.05321755945558289, Accuracy: 0.9935304413051632\n",
      "\n",
      "Correct Prediction (Eval): 2000 out of 9999\n",
      "Validation Loss: 8.425416313934326, Accuracy: 0.20002000200020004\n",
      "\n",
      "Epoch 4 / 4\n",
      "------------------------------------------------------------\n",
      "Correct Prediction (Train): 31037 out of 31996\n",
      "Train Loss: 0.17831975299492478, Accuracy: 0.9700275034379298\n",
      "\n",
      "Correct Prediction (Eval): 2000 out of 9999\n",
      "Validation Loss: 7.330891273498535, Accuracy: 0.20002000200020004\n",
      "\n",
      "Training Time (4 epochs): 4373.52\n",
      "\n",
      "\n",
      "************** Getting predictions **************\n",
      "\n",
      "************** Evaluating performance **************\n",
      "\n",
      "************** Model Evaluation **************\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[   0 7999]\n",
      " [   0 2000]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000      7999\n",
      "           1     0.2000    1.0000    0.3334      2000\n",
      "\n",
      "    accuracy                         0.2000      9999\n",
      "   macro avg     0.1000    0.5000    0.1667      9999\n",
      "weighted avg     0.0400    0.2000    0.0667      9999\n",
      "\n",
      "\n",
      "Output file: 'eval_bert_ratio_balance/eval_bert_balance1_100_scibert_scivocab_cased_abs.txt' Created\n",
      "\n",
      "Total Processing Time: 75min\n",
      "\n",
      "\n",
      "************** Processing Completed **************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    \n",
    "    ###### 1. Set Parameter Values ######\n",
    "\n",
    "    #### 1-1. Input file name & which column\n",
    "    input_filename=\"output_rct.txt\"    \n",
    "    column_name = \"abs\"                                        # 'title' for title text; 'abs' for abstract; 'mix' for title + abstract\n",
    "    \n",
    "    #### 1-2. Data size change?\n",
    "    datachange_on=1                                            # 0 for no change; 1 for change of data size\n",
    "    \n",
    "    ## class balance (1:1)?\n",
    "    balance_on=1                                               # 0 for no balance; 1 for class balance (1:1)\n",
    "    balance_sample_on=1                                        # 0 for no sampling; 1 for sampling\n",
    "    balance_sample_type='under'                                # 'over'(oversampling); 'under'(undersampling)\n",
    "    balance_str = 'balance' + str(balance_on) + '_'\n",
    "    \n",
    "    ## data increase?\n",
    "    ratio_on=1 \n",
    "    ratio_list=[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, \n",
    "                0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]  # basic ratio for data size\n",
    "    #ratio_list=[1000,2000,3000,4000,5000,6000,7000,8000,9000,10000,20000,30000,40000,50000,60000,70000] # actual sample size \n",
    "    #ratio_list=[101]\n",
    "    \n",
    "    #### 1-3. Sampling applied?\n",
    "    sampling_on=0                                              # 0 for no sampling; 1 for sampling\n",
    "    sampling_type='under'                                      # Use when sampling_on=1; 'over'(oversampling), 'under'(undersampling)\n",
    "    \n",
    "    #### 1-4. Which BERT model to use?\n",
    "    #pretrained_model_name = 'bert-base-cased'\n",
    "    #pretrained_model_name = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "    pretrained_model_name = 'allenai/scibert_scivocab_cased'\n",
    "    \n",
    "    # load pretrained tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)  \n",
    "    modelname_string = pretrained_model_name.split(\"/\")[-1] \n",
    "\n",
    "    #### 1-5. Binary or multi classification?\n",
    "    num_class = 2                                              # number of label class\n",
    "    \n",
    "    #### 1-6. Check token distribution for MAX_LEN value: commentize if not needed\n",
    "    #print(\"\\n************** Token Distribution **************\")\n",
    "    #df_token = load_data(input_filename, column_name, record=None)\n",
    "    #token_distribution(df_token, tokenizer)\n",
    "\n",
    "    #### 1-7. Hyperparameters for BERT  \n",
    "    MAX_LEN = 512                                              # 150 for title; 512 for abs (Maximum input size: 512 (BERT))\n",
    "    BATCH_SIZE = 16                                            # Batch size: 16 or 32\n",
    "    EPOCHS = 4                                                 # Number of epochs: 2,3,4\n",
    "    LEARNING_RATE = 2e-5                                       # Learning rate:5e-5, 3e-5, 2e-5\n",
    "\n",
    "    #### 1-8. Evaluation & probability files\n",
    "    eval_on=1                                                  # 0 for no; 1 for yes (display confusion matrix/classification report)\n",
    "    proba_on=0                                                 # 0 for no; 1 for yes (probability output) \n",
    "    \n",
    "        \n",
    "    ###### 2. Run Main Fuction ######\n",
    "\n",
    "    if datachange_on:                  \n",
    "        for ratio in ratio_list:           \n",
    "            if sampling_on:\n",
    "                proba_file = \"result_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".csv\"  \n",
    "                eval_file = \"eval_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".txt\"\n",
    "                model_state_file = \"best_model_state_\" + str(ratio) + \"_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".bin\"\n",
    "            else:\n",
    "                proba_file = \"result_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + column_name + \".csv\"  \n",
    "                eval_file = \"eval_bert_ratio_balance/eval_bert_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + column_name + \".txt\"\n",
    "                model_state_file = \"best_model_state_\" + balance_str + str(ratio) + \"_\" + modelname_string + \"_\" + column_name + \".bin\"\n",
    "        \n",
    "            main(input_file=input_filename, \n",
    "                 colname=column_name,\n",
    "                 sample_on=sampling_on, \n",
    "                 sample_type=sampling_type,\n",
    "                 tokenizer=tokenizer,\n",
    "                 max_len=MAX_LEN, \n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 modelname=modelname_string,\n",
    "                 n_class=num_class,\n",
    "                 device=device,\n",
    "                 pretrained_model=pretrained_model_name,\n",
    "                 learning_rate=LEARNING_RATE,\n",
    "                 epochs=EPOCHS,\n",
    "                 model_file=model_state_file, \n",
    "                 eval_on=eval_on, \n",
    "                 proba_file=proba_file,\n",
    "                 proba_on=proba_on,\n",
    "                 result_file=eval_file,\n",
    "                 datasize_change=datachange_on,\n",
    "                 sample_ratio=ratio_on,\n",
    "                 sample_balance=balance_on,\n",
    "                 balance_sampling_on=balance_sample_on,                                      \n",
    "                 balance_sampling_type=balance_sample_type,\n",
    "                 ratio=ratio)\n",
    "    else:\n",
    "        if sampling_on:\n",
    "            proba_file = \"result_bert_all_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".csv\"  \n",
    "            eval_file = \"eval_bert_all_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".txt\"\n",
    "            model_state_file = \"best_model_state_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".bin\"\n",
    "        else:\n",
    "            proba_file = \"result_bert_all_\" + modelname_string + \"_\" + column_name + \".csv\"  \n",
    "            eval_file = \"eval_bert_all_\" + modelname_string + \"_\" + column_name + \".txt\" \n",
    "            model_state_file = \"best_model_state_\" + modelname_string + \"_\" + column_name + \".bin\"\n",
    "            \n",
    "        main(input_file=input_filename, \n",
    "             colname=column_name,\n",
    "             sample_on=sampling_on, \n",
    "             sample_type=sampling_type,\n",
    "             tokenizer=tokenizer,\n",
    "             max_len=MAX_LEN, \n",
    "             batch_size=BATCH_SIZE,\n",
    "             modelname=modelname_string,\n",
    "             n_class=num_class,\n",
    "             device=device,\n",
    "             pretrained_model=pretrained_model_name,\n",
    "             learning_rate=LEARNING_RATE,\n",
    "             epochs=EPOCHS,\n",
    "             model_file=model_state_file, \n",
    "             eval_on=eval_on, \n",
    "             proba_file=proba_file,\n",
    "             proba_on=proba_on,\n",
    "             result_file=eval_file,\n",
    "             datasize_change=datachange_on,\n",
    "             sample_ratio=ratio_on,\n",
    "             sample_balance=balance_on,\n",
    "             balance_sampling_on=balance_sample_on,                                      \n",
    "             balance_sampling_type=balance_sample_type,\n",
    "             ratio=0.1)\n",
    "        \n",
    "    print(\"\\n************** Processing Completed **************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzR4TEFqs1KX"
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v9QkwvsoOMA"
   },
   "source": [
    "## Download the finetuned model for prediction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "75Iok4xEnExA"
   },
   "outputs": [],
   "source": [
    "# If want to download the pretrained model later, use the following code\n",
    "#n_class = 2\n",
    "#model = LabelClassifier(n_class)\n",
    "#model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HZAHvBRLkPaD"
   },
   "outputs": [],
   "source": [
    "# If you want to download a file directly from Google Drive to your local computer,\n",
    "# uncomment the following code\n",
    "\n",
    "#from google.colab import files\n",
    "#files.download(os.path.join(path, \"result.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMjDW4McjCaD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
