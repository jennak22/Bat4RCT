{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoFLIx-rF-N"
   },
   "source": [
    "# Text Classification using Classical ML algorithms with RCT data\n",
    "\n",
    "Run traditional machine learning models on RCT (Randomized Controlled Trial) data for prediction  \n",
    "\n",
    "Author: Jenna Kim  \n",
    "Created: 2022/1/12  \n",
    "Last Modified: 2022/10/3 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UivnBz2SL4b8"
   },
   "source": [
    "# Update:  \n",
    "- Modify load_data function to read txt file: V2 \n",
    "- Add code to remove duplicates: V2    \n",
    "- Add code to remove stopwords: V2  \n",
    "- Replace stemming with lemmatizing: V2  \n",
    "- Add code to sample data for data size change: V2 \n",
    "- Modify code to sample data for label balance (1:1): V2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjDQGl91wA78"
   },
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1664298398474,
     "user": {
      "displayName": "Jenna Kim",
      "userId": "03479937339783993918"
     },
     "user_tz": 300
    },
    "id": "iCQmTfQRrF-S",
    "outputId": "f8bd2512-35c4-4425-98f8-faffd8ffe628"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3788,
     "status": "ok",
     "timestamp": 1664298402257,
     "user": {
      "displayName": "Jenna Kim",
      "userId": "03479937339783993918"
     },
     "user_tz": 300
    },
    "id": "7OIb10PFrF-U",
    "outputId": "ef4757e6-8cb7-435e-ee88-6661e772aa28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: imbalanced-learn==0.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from imbalanced-learn==0.8.1) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from imbalanced-learn==0.8.1) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from imbalanced-learn==0.8.1) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from imbalanced-learn==0.8.1) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn==0.8.1) (3.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn==1.0.2) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn==1.0.2) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn==1.0.2) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn==1.0.2) (3.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Imbalanced-Learn library for sampling if not already installed\n",
    "\n",
    "#!pip install imbalanced-learn==0.8.1\n",
    "#!pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "acU18CBUv4xQ"
   },
   "outputs": [],
   "source": [
    "# Hide warning messages from display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage \n",
    "#!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear occupied memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage again\n",
    "#!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmfKPvo9wEne"
   },
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZId3r7iLLipc"
   },
   "outputs": [],
   "source": [
    "def load_data_txt(filename, colname, record):\n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file\n",
    "    colname: column name for texts\n",
    "    record: text file to save summary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. Read in data from input file\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", encoding='utf-8', header=None, names=['pmid', 'pubtype', 'year', 'title', 'abstract'])\n",
    "    \n",
    "    # No of rows and columns\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record) \n",
    "    print(\"No of Rows: {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    ## 2. Select data needed for processing & convert labels\n",
    "    df = df[['pmid', 'title', 'abstract', 'pubtype']]\n",
    "\n",
    "    ## 3. Cleaning data \n",
    "    #Trim unnecessary spaces for strings\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: x.strip())\n",
    "    df[\"abstract\"] = df[\"abstract\"].apply(lambda x: x.strip())\n",
    "\n",
    "    # Remove null values \n",
    "    df=df.dropna()\n",
    "\n",
    "    print(\"No of rows (After dropping null): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record)\n",
    "    print(\"No of rows (After dropping null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    # Remove duplicates and keep first occurrence\n",
    "    df.drop_duplicates(subset=['pmid'], keep='first', inplace=True)\n",
    "\n",
    "    print(\"No of rows (After removing duplicates): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of rows (After removing duplicates): {}\".format(df.shape[0]))\n",
    "\n",
    "    ## 4. Select text column\n",
    "    if colname == \"title\":\n",
    "        df = df[['pmid', 'title', 'pubtype']]\n",
    "        df.rename({\"title\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"abs\":\n",
    "        df = df[['pmid', 'abstract', 'pubtype']]\n",
    "        df.rename({\"abstract\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"mix\":\n",
    "        df['mix'] = df[['title','abstract']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
    "        df = df[['pmid', 'mix', 'pubtype']]\n",
    "        df.rename({\"mix\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "\n",
    "    # Check the first few instances\n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head()) \n",
    "    \n",
    "    # No of lables and rows \n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)   \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "\n",
    "    ## 5. Split into X and y\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "     \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pWnA_g5SrF-V"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, colname, record):\n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file\n",
    "    colname: column name for texts\n",
    "    record: text file to save summary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(filename, encoding='utf-8')\n",
    "    \n",
    "    # No of rows and columns\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    print(\"No of Rows: {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "    \n",
    "    # Select data needed for processing & convert labels\n",
    "    df = df[['pmid', 'title', 'abstract', 'pubtype']]\n",
    "    df.iloc[:, -1] = df.iloc[:, -1].map({'RCT':1, 'Other':0})\n",
    "\n",
    "    # Remove null values \n",
    "    df=df.dropna()\n",
    "\n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    # Select text columns\n",
    "    if colname == \"title\":\n",
    "        df = df[['pmid', 'title', 'pubtype']]\n",
    "        df.rename({\"title\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"abs\":\n",
    "        df = df[['pmid', 'abstract', 'pubtype']]\n",
    "        df.rename({\"abstract\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"mix\":\n",
    "        df['mix'] = df[['title','abstract']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
    "        df = df[['pmid', 'mix', 'pubtype']]\n",
    "        df.rename({\"mix\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "\n",
    "    # Check the first few instances\n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head()) \n",
    "    \n",
    "    # No of lables and rows \n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "\n",
    "    # Split into X and y\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "     \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i6UoMg2jrF-W"
   },
   "outputs": [],
   "source": [
    "def sample_data(X_train, y_train, record, sampling=0, sample_method='over'):\n",
    "    \"\"\"\n",
    "       Sampling input train data\n",
    "       \n",
    "       X_train: dataframe of X train data\n",
    "       y_train: datafram of y train data\n",
    "       sampling: indicator of sampling funtion is on or off\n",
    "       sample_method: method of sampling (oversampling or undersampling)\n",
    "       record: text file to save summary\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    if sampling:\n",
    "        # select a sampling method\n",
    "        if sample_method == 'over':\n",
    "            #oversample = RandomOverSampler(random_state=42)\n",
    "            oversample = RandomOverSampler(random_state=101)\n",
    "            X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******', file=record)\n",
    "            print('\\n****** Data Sampling ******')\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()), file=record)\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_over, y_over\n",
    "            \n",
    "        elif sample_method == 'under':\n",
    "            #undersample = RandomUnderSampler(random_state=42)\n",
    "            undersample = RandomUnderSampler(random_state=101)\n",
    "            X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******', file=record)\n",
    "            print('\\n****** Data Sampling ******')\n",
    "            print('\\nUndersampled Data (class,Rows):\\n{}'.format(y_under.value_counts()), file=record)\n",
    "            print('\\nUndersampled Data (class,Rows):\\n{}'.format(y_under.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_under, y_under\n",
    "    else:\n",
    "        X_train_sam, y_train_sam = X_train, y_train \n",
    "        print('\\n****** Data Sampling ******', file=record)\n",
    "        print('\\n****** Data Sampling ******')\n",
    "        print('\\nNo Sampling Performed\\n', file=record)\n",
    "        print('\\nNo Sampling Performed\\n')\n",
    "    \n",
    "    return X_train_sam, y_train_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Kx6Q8w3lrF-X"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_data_raw):\n",
    "    \"\"\"\n",
    "       Preprocess data with lowercase conversion, punctuation removal, tokenization, stemming\n",
    "       \n",
    "       X_data_raw: X data in dataframe\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    X_data=X_data_raw.iloc[:, -1].astype(str)\n",
    "   \n",
    "    # 1. convert all characters to lowercase\n",
    "    X_data = X_data.map(lambda x: x.lower())\n",
    "    \n",
    "    # 2. remove punctuation\n",
    "    X_data = X_data.str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    # 3. tokenize sentence\n",
    "    X_data = X_data.apply(nltk.word_tokenize)\n",
    "\n",
    "    # 4. remove stopwords\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    X_data = X_data.apply(lambda x: [word for word in x if word not in stopword_list])\n",
    "\n",
    "    print(\"\\n<Data tokenzized and stopwords removed>\\n\", X_data)\n",
    "\n",
    "    # 5. lemmatize\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #X_data = X_data.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "    \n",
    "    # 5. stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    X_data = X_data.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    \n",
    "    # 6. removing unnecessary space\n",
    "    X_data = X_data.apply(lambda x: \" \".join(x)) \n",
    "    \n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Nnyw5kYrF-X"
   },
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train, model='DT'):\n",
    "    \n",
    "    \"\"\"\n",
    "      Model fitting with options of classifiers:\n",
    "      decision tree, svm, knn, naive bayes, random forest, and gradient boosting\n",
    "      \n",
    "      X_train: X train data\n",
    "      y_train: y train data\n",
    "      model: name of classifier\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    if model=='DT':\n",
    "        DT = DecisionTreeClassifier(max_depth=2)\n",
    "        model = DT.fit(X_train, y_train)\n",
    "    elif model=='SVM':\n",
    "        SVM = SVC(kernel='linear', probability=True)  \n",
    "        model = SVM.fit(X_train, y_train)\n",
    "    elif model=='NB':\n",
    "        NB = MultinomialNB()\n",
    "        model = NB.fit(X_train, y_train)\n",
    "    elif model=='LR':\n",
    "        LR = LogisticRegression()\n",
    "        model = LR.fit(X_train, y_train)   \n",
    "    elif model=='RF':\n",
    "        RF = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "        model = RF.fit(X_train, y_train)\n",
    "    elif model=='GB':\n",
    "        GB = GradientBoostingClassifier()\n",
    "        model = GB.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ctZ80-2prF-Y"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred, record, eval_model=0):\n",
    "    \"\"\"\n",
    "      evaluate model performance\n",
    "      \n",
    "      y_test: y test data\n",
    "      y_pred: t prediction score\n",
    "      eval_model: indicator if this funtion is on or off\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    if eval_model:\n",
    "        print('\\n************** Model Evaluation **************', file=record)\n",
    "        print('\\n************** Model Evaluation **************')\n",
    "        \n",
    "        print('\\nConfusion Matrix:\\n', file=record)\n",
    "        print(confusion_matrix(y_test, y_pred), file=record)\n",
    "        print('\\nConfusion Matrix:\\n')\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "        print('\\nClassification Report:\\n', file=record)\n",
    "        print(classification_report(y_test, y_pred, digits=4), file=record)\n",
    "        print('\\nClassification Report:\\n')\n",
    "        print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H8ikM_EPrF-Z"
   },
   "outputs": [],
   "source": [
    "def predict_proba(model, X_test_trans, X_test, y_test, y_pred, proba_file, proba_out=0):\n",
    "    \"\"\"\n",
    "       Predict probability of each class\n",
    "       \n",
    "       model: trained model with a selected classifier\n",
    "       X_test_trans: X test data preprocessed\n",
    "       X_test: original X test data\n",
    "       y_test: original y test data\n",
    "       y_pred: predicted y values\n",
    "       proba_file: output file of probability scores\n",
    "       proba_out: decide if the probability output is expected\n",
    "       \n",
    "    \"\"\"\n",
    "    if proba_out:\n",
    "      \n",
    "        ## Compute probability\n",
    "        y_prob = model.predict_proba(X_test_trans)\n",
    "        df_prob = pd.DataFrame(data=y_prob, columns=model.classes_)\n",
    "        result = pd.concat([X_test.reset_index(drop=True), df_prob], axis=1, ignore_index=False)\n",
    "    \n",
    "        ## Add predicted class to output\n",
    "        result['pred'] = pd.Series(y_pred)\n",
    "\n",
    "        ## Add actual class to output \n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        result['act'] = y_test\n",
    "\n",
    "        ## Save output\n",
    "        result.to_csv(proba_file, encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtuA57AQwTjA"
   },
   "source": [
    "# 3. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5YROMNEZrF-Z"
   },
   "outputs": [],
   "source": [
    "def main(input_file, \n",
    "         colname,   \n",
    "         sample_on, \n",
    "         sample_type, \n",
    "         model_method, \n",
    "         eval_on, \n",
    "         proba_file,\n",
    "         proba_on,\n",
    "         result_file,\n",
    "         datasize_change,\n",
    "         sample_balance,\n",
    "         balance_sampling_on,                                   \n",
    "         balance_sampling_type,\n",
    "         sample_ratio,\n",
    "         ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "       Main function for processing data, model fitting, and prediction\n",
    "       \n",
    "       input_file: input file\n",
    "       colname: colume name for selection between title and abstract\n",
    "       sample_on: indicator of sampling on or off\n",
    "       sample_type: sample type to choose if sample_on is 1\n",
    "       model_method: name of classifier to be applied for model fitting\n",
    "       eval_on: indicator of model evaluation on or off\n",
    "       proba_file: name of output file of probability\n",
    "       proba_on: indicator of getting probability\n",
    "       result_file: name of output file of evaluation\n",
    "       datasize_change: indication of data size change\n",
    "       ratio: proportion of data size\n",
    "       \n",
    "    \"\"\"\n",
    "    ## 0. open result file for records\n",
    "    f=open(result_file, \"a\")\n",
    "    \n",
    "    # Check processing time\n",
    "    proc_start_time = timeit.default_timer()\n",
    "    \n",
    "    ## 1. Load data\n",
    "    \n",
    "    print(\"\\n************** Loading Data ************\\n\", file=f)\n",
    "    print(\"\\n************** Loading Data ************\\n\")\n",
    "    #X, y = load_data(input_file, colname, record=f)         # use for comma-delimited csv file\n",
    "    X, y = load_data_txt(input_file, colname, record=f)      # use for tab-delimited txt file\n",
    "    \n",
    "    # testing\n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]), file=f)\n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]))\n",
    "\n",
    "    ## 2. Train and test split\n",
    "    \n",
    "    print(\"\\n************** Spliting Data **************\\n\", file=f)\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test,y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "    \n",
    "    # For testing only: small size data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=42, stratify=y)\n",
    "    #X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.99, random_state=42, stratify=y_test)\n",
    "    #X_notuse, X_test, y_notuse, y_test = train_test_split(X_test, y_test, test_size=0.01, random_state=42, stratify=y_test)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(X_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape))\n",
    "    print(\"Val Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "    \n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "    \n",
    "    ## 3. Data size change\n",
    "    \n",
    "    if datasize_change:\n",
    "        \n",
    "        # Sample size reduce: 500,000 instance -> 100,000 instance\n",
    "        X_train, _, y_train, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42, stratify=y_train)\n",
    "        X_test, _, y_test, _ = train_test_split(X_test, y_test, train_size=0.2, random_state=42, stratify=y_test)\n",
    "        \n",
    "        print(\"\\n************** Data Size Change: Reducing Data **************\\n\", file=f)\n",
    "        print(\"\\n************** Data Size Change: Reducing Data **************\\n\")\n",
    "        print(\"Train Data: {}\".format(X_train.shape), file=f)\n",
    "        print(\"Test Data: {}\".format(X_test.shape), file=f) \n",
    "        print(\"Train Data: {}\".format(X_train.shape))\n",
    "        print(\"Test Data: {}\".format(X_test.shape)) \n",
    "        \n",
    "        print('\\nClass Counts(label, row): Train', file=f)\n",
    "        print(y_train.value_counts(), file=f)\n",
    "        print('\\nClass Counts(label, row): Test', file=f)\n",
    "        print(y_test.value_counts(), file=f)\n",
    "        print('\\nClass Counts(label, row): Train')\n",
    "        print(y_train.value_counts())\n",
    "        print('\\nClass Counts(label, row): Test')\n",
    "        print(y_test.value_counts())\n",
    "        \n",
    "        print(\"\\n<X_train Data>\", file=f)\n",
    "        print(X_train.head(), file=f)\n",
    "        print(\"\\n<X_train Data>\")\n",
    "        print(X_train.head())\n",
    "    \n",
    "        print(\"\\n<X_test Data>\", file=f)\n",
    "        print(X_test.head(), file=f)\n",
    "        print(\"\\n<X_test Data>\")\n",
    "        print(X_test.head())     \n",
    "        \n",
    "        # Sample data with balance (1:1)\n",
    "        if sample_balance:\n",
    "            \n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\", file=f)\n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\")\n",
    "            \n",
    "            X_train, y_train = sample_data(X_train, y_train, record=f, \n",
    "                                           sampling=balance_sampling_on, \n",
    "                                           sample_method=balance_sampling_type)\n",
    "                      \n",
    "            print('\\nClass Counts(label, row): After balancing', file=f)\n",
    "            print(y_train.value_counts(), file=f)\n",
    "            print('\\nClass Counts(label, row): After balancing')\n",
    "            print(y_train.value_counts())\n",
    "            print(\"\\n<Balanced Train Data>\", file=f)\n",
    "            print(X_train.head(), file=f)\n",
    "            print(\"\\n<Balanced Train Data>\")\n",
    "            print(X_train.head()) \n",
    "                  \n",
    "        # Sample data based on size ratio    \n",
    "        if sample_ratio:\n",
    "            if ratio == 1:\n",
    "                X_train = X_train\n",
    "                y_train = y_train       \n",
    "            else:\n",
    "                X_train, _, y_train, _ = train_test_split(X_train, y_train, train_size=ratio, \n",
    "                                                          random_state=42, stratify=y_train)\n",
    "                \n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\", file=f)\n",
    "            print(\"Data Ratio: {}\".format(ratio), file=f)\n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\")\n",
    "            print(\"Data Ratio: {}\".format(ratio))\n",
    "     \n",
    "            print('\\nClass Counts(label, row): After sampling', file=f)\n",
    "            print(y_train.value_counts(), file=f)\n",
    "            print('\\nClass Counts(label, row): After sampling')\n",
    "            print(y_train.value_counts())\n",
    "            print(\"\\n<Train Data Based on Ratio>\", file=f)\n",
    "            print(X_train.head(), file=f)\n",
    "            print(\"\\n<Train Data Based on Ratio>\")\n",
    "            print(X_train.head())\n",
    "        \n",
    "    # Reset index\n",
    "    X_train=X_train.reset_index(drop=True)\n",
    "    X_test=X_test.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test=y_test.reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n************** Processing Data **************\", file=f)\n",
    "    print(\"\\n************** Processing Data **************\")\n",
    "    print(\"\\nTrain Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    print(\"\\nTrain Data: {}\".format(X_train.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(y_train.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(y_test.value_counts())\n",
    "    \n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "    \n",
    "    ## 4. Sampling \n",
    "    X_train_samp, y_train_samp = sample_data(X_train, y_train, record=f, sampling=sampling_on, sample_method=sample_type)\n",
    "    \n",
    "    ## 5. Preprocessing \n",
    "    X_train_pro = preprocess_data(X_train_samp)\n",
    "    \n",
    "    print(\"\\n<After preprocessing training data>\", file=f)\n",
    "    print(X_train_pro, file=f)\n",
    "    print(\"\\n<After preprocessing training data>\")\n",
    "    print(X_train_pro)\n",
    "    \n",
    "    # TFIDF transformation\n",
    "    count_vect = CountVectorizer()\n",
    "    counts = count_vect.fit_transform(X_train_pro)\n",
    "    transformer = TfidfTransformer(smooth_idf=True, use_idf=True).fit(counts)\n",
    "    X_train_transformed = transformer.transform(counts)\n",
    "    \n",
    "    X_train_trans = X_train_transformed\n",
    "    y_train_trans = y_train_samp\n",
    "\n",
    "    ## 6. Model Fitting\n",
    "    print(\"\\n************** Training Model: \" + model_method + \" **************\", file=f)\n",
    "    print(\"\\n************** Training Model: \" + model_method + \" **************\")\n",
    "\n",
    "    # Check training time\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Fit the model\n",
    "    model = fit_model(X_train_trans, y_train_trans, model=model_method)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"\\nTraining Time: {}\".format(round(elapsed, 2)), file=f)\n",
    "    print(\"\\nTraining Time: {}\".format(round(elapsed,2)))\n",
    "\n",
    "    ## 7. Prediction\n",
    "    print(\"\\n\\n************** Getting predictions **************\", file=f)\n",
    "    print(\"\\n\\n************** Getting predictions **************\")\n",
    "\n",
    "    # Transform X_test data\n",
    "    X_test_pro = preprocess_data(X_test)\n",
    "    counts_test = count_vect.transform(X_test_pro)\n",
    "    X_test_trans = transformer.transform(counts_test)\n",
    "    \n",
    "    # Predict output\n",
    "    y_pred = model.predict(X_test_trans)\n",
    "    \n",
    "    ## 8. Evaluating model performance\n",
    "    print(\"\\n************** Evaluating performance **************\", file=f)\n",
    "    print(\"\\n************** Evaluating performance **************\")\n",
    "    evaluate_model(y_test, y_pred, record=f, eval_model=eval_on)\n",
    "    \n",
    "    ## 9. Probability prediction    \n",
    "    predict_proba(model, X_test_trans, X_test, y_test, y_pred, proba_file=proba_file, proba_out=proba_on)\n",
    "    \n",
    "    print(\"\\nOutput file:'\" + result_file + \"' Created\", file=f)\n",
    "    print(\"\\nOutput file:'\" + result_file + \"' Created\")\n",
    "    \n",
    "    # Checking processing time\n",
    "    proc_elapsed = timeit.default_timer() - proc_start_time\n",
    "    print(\"\\nTotal Processing Time: {}min\\n\".format(round(proc_elapsed/60)), file=f)\n",
    "    print(\"\\nTotal Processing Time: {}min\\n\".format(round(proc_elapsed/60)))\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYwmardMwKPF"
   },
   "source": [
    "# 4. Run code for implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYt_qbIcrF-a",
    "outputId": "a6c46358-5df9-4f8f-8c35-d3e9afbd66cf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Loading Data ************\n",
      "\n",
      "No of Rows: 500068\n",
      "No of Columns: 5\n",
      "No of rows (After dropping null): 500068\n",
      "No of columns: 4\n",
      "No of rows (After removing duplicates): 499963\n",
      "\n",
      "<Data View: First Few Instances>\n",
      "\n",
      "        pmid                                           sentence  label\n",
      "0  18439781  In the United States, an increasing number of ...      0\n",
      "1  18468833  The American Heart Association website defines...      0\n",
      "2  18481181  The complex pathophysiology of traumatic brain...      0\n",
      "3  18728056  [BACKGROUND] Soluble CD40 ligand (sCD40L) is a...      1\n",
      "4  18790590  [BACKGROUND] Internal carotid artery dissectio...      0\n",
      "\n",
      "Class Counts(label, row): Total\n",
      "0    399977\n",
      "1     99986\n",
      "Name: label, dtype: int64\n",
      "\n",
      "<First Sentence>\n",
      "In the United States, an increasing number of law enforcement agencies have employed the use of TASER® (TASER International Inc., Scottsdale, AZ) devices to temporarily immobilize violent subjects. There are reports in the lay press of adverse outcomes occurring in patients on whom TASER® devices have been deployed. Rhabdomyolysis has been associated with patients sustaining a TASER® shock, with a 1% incidence rate in subjects subdued with earlier versions of the device and then brought to the Emergency Department (ED). We present the cases of 2 patients who were seen in our ED after exhibiting violent behavior and receiving TASER® shocks. Both were hospitalized and received treatment for mild rhabdomyolysis. Both patients had multiple other characteristics that have been found to have an association with the development of rhabdomyolysis, in addition to the shocks they received. A review and discussion of the available medical literature on the subject follows, describing several complications that have been documented in patients after receiving TASER® shocks. Although a direct link between the TASER® and the reported adverse effects has not been established, patients who undergo restraint via this device frequently have pre-existing conditions or have exhibited behavior that places them at risk for the development of those effects. Such awareness of these possible complications is vital because the evaluation and management of patients developing adverse effects after these events will commonly occur in the ED.\n",
      "\n",
      "************** Spliting Data **************\n",
      "\n",
      "Train Data: (399970, 2)\n",
      "Val Data: (49996, 2)\n",
      "Test Data: (49997, 2)\n",
      "\n",
      "<X_test Data>\n",
      "            pmid                                           sentence\n",
      "141906  24081098  We explain a technique that recovers the struc...\n",
      "5372    21075537  Post-burn pruritis is a very distressing sympt...\n",
      "251994  26642726  Auditory or visual hallucinations of a decease...\n",
      "10967   21255258  [PURPOSE] The study aims to assess indicators ...\n",
      "266669  26996521  Mid-Late Holocene environmental changes in the...\n",
      "\n",
      "************** Data Size Change: Reducing Data **************\n",
      "\n",
      "Train Data: (79994, 2)\n",
      "Test Data: (9999, 2)\n",
      "\n",
      "Class Counts(label, row): Train\n",
      "0    63996\n",
      "1    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Test\n",
      "0    7999\n",
      "1    2000\n",
      "Name: label, dtype: int64\n",
      "\n",
      "<X_train Data>\n",
      "            pmid                                           sentence\n",
      "132892  23883649  [INTRODUCTION] The objective of this article w...\n",
      "432914  31293160  Inhibiting starch digestion can effectively co...\n",
      "202418  25490610  [BACKGROUND] The prevalence of hepatitis-C-vir...\n",
      "465499  32186175  Plastic materials contain various additives, w...\n",
      "494695  33218043  Tobacco (Nicotiana tabacum) pollen is a well-s...\n",
      "\n",
      "<X_test Data>\n",
      "            pmid                                           sentence\n",
      "339817  28895087  [BACKGROUND] This study aimed to quantify the ...\n",
      "445141  31610995  [OBJECTIVES] The benefit of alerting clinical ...\n",
      "37112   21824051  We evaluated sequential bortezomib, liposomal ...\n",
      "271607  27115241  [BACKGROUND] Hearing loss and dementia are bot...\n",
      "287114  27526991  [OBJECTIVE] Our study objective was to determi...\n",
      "\n",
      "************** Data Balancing: Label Class (1:1) *************\n",
      "\n",
      "\n",
      "****** Data Sampling ******\n",
      "\n",
      "Undersampled Data (class,Rows):\n",
      "0    15998\n",
      "1    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): After balancing\n",
      "0    15998\n",
      "1    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "<Balanced Train Data>\n",
      "       pmid                                           sentence\n",
      "0  26304469  Esophageal diverticula are rare findings that ...\n",
      "1  23378388  The patient was a 63-year-old female who had a...\n",
      "2  31246674  [BACKGROUND] The purpose of this article is to...\n",
      "3  28108738  Owing to the central role of apoptosis in many...\n",
      "4  25713042  A Gram-stain-negative, non-motile, aerobic and...\n",
      "\n",
      "************** Processing Data **************\n",
      "\n",
      "Train Data: (31996, 2)\n",
      "Test Data: (9999, 2)\n",
      "\n",
      "Class Counts(label, row): Train\n",
      "0    15998\n",
      "1    15998\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Class Counts(label, row): Test\n",
      "0    7999\n",
      "1    2000\n",
      "Name: label, dtype: int64\n",
      "\n",
      "<X_test Data>\n",
      "       pmid                                           sentence\n",
      "0  28895087  [BACKGROUND] This study aimed to quantify the ...\n",
      "1  31610995  [OBJECTIVES] The benefit of alerting clinical ...\n",
      "2  21824051  We evaluated sequential bortezomib, liposomal ...\n",
      "3  27115241  [BACKGROUND] Hearing loss and dementia are bot...\n",
      "4  27526991  [OBJECTIVE] Our study objective was to determi...\n",
      "\n",
      "****** Data Sampling ******\n",
      "\n",
      "No Sampling Performed\n",
      "\n",
      "\n",
      "<Data tokenzized and stopwords removed>\n",
      " 0        [esophageal, diverticula, rare, findings, esti...\n",
      "1        [patient, 63yearold, female, past, history, hy...\n",
      "2        [background, purpose, article, develop, new, m...\n",
      "3        [owing, central, role, apoptosis, many, human,...\n",
      "4        [gramstainnegative, nonmotile, aerobic, coccoi...\n",
      "                               ...                        \n",
      "31991    [background, semaglutide, novel, glucagonlike,...\n",
      "31992    [purpose, investigate, effect, xinjingjie, com...\n",
      "31993    [aims, inhibition, neutral, endopeptidases, ne...\n",
      "31994    [background, evaluate, efficacy, newly, develo...\n",
      "31995    [objectives, externally, validate, compare, tw...\n",
      "Name: sentence, Length: 31996, dtype: object\n",
      "\n",
      "<After preprocessing training data>\n",
      "0        esophag diverticula rare find estim incid 1 pe...\n",
      "1        patient 63yearold femal past histori hypertens...\n",
      "2        background purpos articl develop new method el...\n",
      "3        owe central role apoptosi mani human diseas wi...\n",
      "4        gramstainneg nonmotil aerob coccoid ovoid rods...\n",
      "                               ...                        \n",
      "31991    background semaglutid novel glucagonlik peptid...\n",
      "31992    purpos investig effect xinjingji compound lyso...\n",
      "31993    aim inhibit neutral endopeptidas nep result be...\n",
      "31994    background evalu efficaci newli develop electr...\n",
      "31995    object extern valid compar two novel version e...\n",
      "Name: sentence, Length: 31996, dtype: object\n",
      "\n",
      "************** Training Model: GB **************\n",
      "\n",
      "Training Time: 104.39\n",
      "\n",
      "\n",
      "************** Getting predictions **************\n",
      "\n",
      "<Data tokenzized and stopwords removed>\n",
      " 0       [background, study, aimed, quantify, benefit, ...\n",
      "1       [objectives, benefit, alerting, clinical, staf...\n",
      "2       [evaluated, sequential, bortezomib, liposomal,...\n",
      "3       [background, hearing, loss, dementia, prevalen...\n",
      "4       [objective, study, objective, determine, feasi...\n",
      "                              ...                        \n",
      "9994    [backgroundaims, ribonucleotide, reductase, su...\n",
      "9995    [objective, study, aimed, determine, frequency...\n",
      "9996    [activation, strain, distortioninteraction, mo...\n",
      "9997    [background, treatment, abdominal, wall, metas...\n",
      "9998    [clinical, application, value, investigated, t...\n",
      "Name: sentence, Length: 9999, dtype: object\n",
      "\n",
      "************** Evaluating performance **************\n",
      "\n",
      "************** Model Evaluation **************\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[7564  435]\n",
      " [ 193 1807]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9751    0.9456    0.9601      7999\n",
      "           1     0.8060    0.9035    0.8520      2000\n",
      "\n",
      "    accuracy                         0.9372      9999\n",
      "   macro avg     0.8905    0.9246    0.9060      9999\n",
      "weighted avg     0.9413    0.9372    0.9385      9999\n",
      "\n",
      "\n",
      "Output file:'eval_ML_ratio_balance/eval_ML_balance1_100_GB_abs.txt' Created\n",
      "\n",
      "Total Processing Time: 4min\n",
      "\n",
      "\n",
      "************** Processing Completed **************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    \n",
    "    ###### 1. Set Parameter Values ######\n",
    "    \n",
    "    #### 1-1. Input file name & which column\n",
    "    #input_filename=\"rct_sample.csv\"  \n",
    "    input_filename=\"output_rct.txt\" \n",
    "    column_name = \"abs\"                                        # 'title' for title text; 'abs' for abstract; 'mix' for title + abstract\n",
    "\n",
    "    #### 1-2. Data size change?\n",
    "    datachange_on=1                                            # 0 for no change; 1 for change of data size\n",
    "    \n",
    "    ## class balance (1:1)?\n",
    "    balance_on=1                                               # 0 for no balance; 1 for class balance (1:1)\n",
    "    balance_sample_on=1                                        # 0 for no sampling; 1 for sampling\n",
    "    balance_sample_type='under'                                # 'over'(oversampling); 'under'(undersampling)\n",
    "    balance_str = 'balance' + str(balance_on) + '_'\n",
    "    \n",
    "    ## data increase?\n",
    "    ratio_on=0 \n",
    "    #ratio_list=[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, \n",
    "    #            0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]   # basic ratio for data size\n",
    "    #ratio_list=[1000,2000,3000,4000,5000,6000,7000,8000,9000,10000,20000,30000,40000,50000,60000,70000]   # sample size\n",
    "    ratio_list=[100]\n",
    "    \n",
    "    #### 1-3. Sampling applied?\n",
    "    sampling_on=0                                              # 0 for no sampling; 1 for sampling\n",
    "    sampling_type='over'                                       # Use when sampling_on=1; 'over'(oversampling), 'under'(undersampling)\n",
    "    \n",
    "    #### 1-4. Which model to use?\n",
    "    model_type='GB'                                            # 'LR'(Logisitic regression);SVM'(SVM);'GB'(Gradient Boosting);\n",
    "                                                               # 'NB'(Naive Bayes);'RF'(Random Forest);\n",
    "    #### 1-5. Evaluation & probability file    \n",
    "    eval_on=1                                                  # 0 for no; 1 for yes (confusion matrix/classification report)\n",
    "    proba_on=0                                                 # 0 for no; 1 for yes (probability output)\n",
    "    \n",
    "    \n",
    "    ###### 2. Run Main Fuction ######\n",
    "\n",
    "    if datachange_on:            \n",
    "        \n",
    "        for ratio in ratio_list:           \n",
    "            if sampling_on:\n",
    "                proba_file = \"result_ML_\" + balance_str + str(ratio) + \"_\" +  model_type + \"_\" + sampling_type + \"_\" + column_name + \".csv\" \n",
    "                eval_file = \"eval_ML_\" + balance_str + str(ratio) + \"_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".txt\" \n",
    "            else:\n",
    "                proba_file = \"result_ML_\" + balance_str + str(ratio) + \"_\" + model_type + \"_\" + column_name + \".csv\"   \n",
    "                eval_file = \"eval_ML_ratio_balance/eval_ML_\" + balance_str + str(ratio) + \"_\" + model_type + \"_\" + column_name + \".txt\"\n",
    "            \n",
    "            main(input_file=input_filename,\n",
    "                 colname=column_name, \n",
    "                 sample_on=sampling_on, \n",
    "                 sample_type=sampling_type,\n",
    "                 model_method=model_type, \n",
    "                 eval_on=eval_on, \n",
    "                 proba_file=proba_file,\n",
    "                 proba_on=proba_on,\n",
    "                 result_file=eval_file,\n",
    "                 datasize_change=datachange_on,\n",
    "                 sample_ratio=ratio_on,\n",
    "                 sample_balance=balance_on,\n",
    "                 balance_sampling_on=balance_sample_on,                                      \n",
    "                 balance_sampling_type=balance_sample_type,\n",
    "                 ratio=ratio)\n",
    "    else:\n",
    "        if sampling_on:\n",
    "            proba_file = \"result_ML_all_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".csv\"    \n",
    "            eval_file = \"eval_ML_all_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".txt\" \n",
    "        else:\n",
    "            proba_file = \"result_ML_all_\" + model_type + \"_\" + column_name + \".csv\" \n",
    "            eval_file = \"eval_ML_all_\" + model_type + \"_\" + column_name + \".txt\" \n",
    "            \n",
    "        main(input_file=input_filename, \n",
    "             colname=column_name,\n",
    "             sample_on=sampling_on, \n",
    "             sample_type=sampling_type,\n",
    "             model_method=model_type, \n",
    "             eval_on=eval_on, \n",
    "             proba_file=proba_file,\n",
    "             proba_on=proba_on,\n",
    "             result_file=eval_file,\n",
    "             datasize_change=datachange_on,\n",
    "             sample_ratio=ratio_on,\n",
    "             sample_balance=balance_on,\n",
    "             balance_sampling_on=balance_sample_on,                                      \n",
    "             balance_sampling_type=balance_sample_type,\n",
    "             ratio=1)\n",
    "        \n",
    "    print(\"\\n************** Processing Completed **************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QftyM5zKrF-b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTixa_JMIpPw"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
